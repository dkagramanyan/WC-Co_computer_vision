{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6136bab-dbfd-44b3-ac6a-186ac448461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "\n",
    "except ImportError:\n",
    "    amp = None\n",
    "\n",
    "# from dataset import LMDBDataset\n",
    "from pixelsnail import PixelSNAIL\n",
    "from scheduler import CycleScheduler\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from skimage import io\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# from scheduler import CycleScheduler\n",
    "from pt_utils import  Embeddings, Trainer, VQVAE, data_sampler, Vqvae2Adaptive\n",
    "from torch.utils import data\n",
    "from torch import distributed as dist\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from skimage import transform, metrics\n",
    "import skimage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "seed = 51\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f91712-7344-4716-b9a0-2e7b35e67828",
   "metadata": {},
   "source": [
    "# Create embeddings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f99f8-1aa8-431d-9d44-1ac9549350a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CodeRow = namedtuple('CodeRow', ['top', 'bottom', 'filename'])\n",
    "\n",
    "\n",
    "class ImageFileDataset(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        sample, target = super().__getitem__(index)\n",
    "        path, _ = self.samples[index]\n",
    "        dirs, filename = os.path.split(path)\n",
    "        _, class_name = os.path.split(dirs)\n",
    "        filename = os.path.join(class_name, filename)\n",
    "\n",
    "        return sample, target, filename\n",
    "\n",
    "\n",
    "class LMDBDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.env = lmdb.open(\n",
    "            path,\n",
    "            max_readers=32,\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "            readahead=False,\n",
    "            meminit=False,\n",
    "        )\n",
    "\n",
    "        if not self.env:\n",
    "            raise IOError('Cannot open lmdb dataset', path)\n",
    "\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            self.length = int(txn.get('length'.encode('utf-8')).decode('utf-8'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            key = str(index).encode('utf-8')\n",
    "\n",
    "            row = pickle.loads(txn.get(key))\n",
    "\n",
    "        return torch.from_numpy(row.top), torch.from_numpy(row.bottom), row.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb32d14-1f6c-4fae-982b-3f5f63cbc14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = '../data/dataset_512/'\n",
    "# dataset_path = '../datasets/bc_right_sub_left_minmax_4x_360'\n",
    "# dataset_path = '../datasets/bc_left_4x_360'\n",
    "# dataset_path = '../datasets/original/o_bc_left_9x_512_360'\n",
    "dataset_path = '../datasets/original/o_bc_left_4x_768'\n",
    "\n",
    "new_dataset_path='../datasets/original/o_bc_left_4x_768_embs'\n",
    "\n",
    "resize_shape = (512, 512)\n",
    "# resize_shape = (1024, 1024)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.Resize(resize_shape),\n",
    "        # transforms.CenterCrop(resize_shape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "vqvae2_model\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "device='cuda'\n",
    "\n",
    "images_embs_t = []\n",
    "images_embs_b = []\n",
    "\n",
    "dataset_path = dataset.__dict__['root']\n",
    "classes_folders = os.listdir(dataset_path)\n",
    "classes_folders_images = [os.listdir(dataset_path + '/' + folder) for folder in classes_folders]\n",
    "classes_folders_images_num = [len(os.listdir(dataset_path + '/' + folder)) for folder in classes_folders]\n",
    "\n",
    "img_transform = dataset.__dict__['transform']\n",
    "\n",
    "for i in range(len(classes_folders)):\n",
    "    print(f'Number of folders {i + 1}/{len(classes_folders)}')\n",
    "    for j in tqdm(range(classes_folders_images_num[i]), desc=f'Folder {classes_folders[i]}'):\n",
    "        image_path = dataset_path + '/' + classes_folders[i] + '/' + classes_folders_images[i][j]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = img_transform(image)\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        image = transform(image)\n",
    "\n",
    "        vqvae2_model.zero_grad()\n",
    "\n",
    "        quant_t, quant_b, _, _, _ = vqvae2_model.encode(image)\n",
    "\n",
    "        torch.save(quant_t,new_dataset_path+'/top_'+classes_folders_images[i][j])\n",
    "        torch.save(quant_b,new_dataset_path+'/bottom_'+classes_folders_images[i][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f99bf8-b7c8-4085-9fef-ccdb571d5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import lmdb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import ImageFileDataset, CodeRow\n",
    "from vqvae import VQVAE\n",
    "\n",
    "\n",
    "def extract(lmdb_env, loader, model, device):\n",
    "    index = 0\n",
    "\n",
    "    with lmdb_env.begin(write=True) as txn:\n",
    "        pbar = tqdm(loader)\n",
    "\n",
    "        for img, _, filename in pbar:\n",
    "            img = img.to(device)\n",
    "\n",
    "            _, _, _, id_t, id_b = model.encode(img)\n",
    "            id_t = id_t.detach().cpu().numpy()\n",
    "            id_b = id_b.detach().cpu().numpy()\n",
    "\n",
    "            for file, top, bottom in zip(filename, id_t, id_b):\n",
    "                row = CodeRow(top=top, bottom=bottom, filename=file)\n",
    "                txn.put(str(index).encode('utf-8'), pickle.dumps(row))\n",
    "                index += 1\n",
    "                pbar.set_description(f'inserted: {index}')\n",
    "\n",
    "        txn.put('length'.encode('utf-8'), str(index).encode('utf-8'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--size', type=int, default=256)\n",
    "    parser.add_argument('--ckpt', type=str)\n",
    "    parser.add_argument('--name', type=str)\n",
    "    parser.add_argument('path', type=str)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    device = 'cuda'\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(args.size),\n",
    "            transforms.CenterCrop(args.size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = ImageFileDataset(args.path, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = VQVAE()\n",
    "    model.load_state_dict(torch.load(args.ckpt))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    map_size = 100 * 1024 * 1024 * 1024\n",
    "\n",
    "    env = lmdb.open(args.name, map_size=map_size)\n",
    "\n",
    "    extract(env, loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de832c55-d8f4-48f1-b046-51e01b1663cb",
   "metadata": {},
   "source": [
    "# Train pixelsnail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebc558fd-bb3a-4263-8631-7f50fe7792e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelTransform:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, input):\n",
    "        ar = np.array(input)\n",
    "\n",
    "        return torch.from_numpy(ar).long()\n",
    "\n",
    "def train(args, epoch, loader, model, optimizer, scheduler, device):\n",
    "    loader = tqdm(loader)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i, (top, bottom, label) in enumerate(loader):\n",
    "        model.zero_grad()\n",
    "\n",
    "        top = top.to(device)\n",
    "\n",
    "        if args.hier == 'top':\n",
    "            target = top\n",
    "            out, _ = model(top)\n",
    "\n",
    "        elif args.hier == 'bottom':\n",
    "            bottom = bottom.to(device)\n",
    "            target = bottom\n",
    "            out, _ = model(bottom, condition=top)\n",
    "\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, pred = out.max(1)\n",
    "        correct = (pred == target).float()\n",
    "        accuracy = correct.sum() / target.numel()\n",
    "\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        loader.set_description(\n",
    "            (\n",
    "                f'epoch: {epoch + 1}; loss: {loss.item():.5f}; '\n",
    "                f'acc: {accuracy:.5f}; lr: {lr:.5f}'\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf86b5-b36d-43f0-95de-5ba55518066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=32\n",
    "epoch=420\n",
    "hier='top'\n",
    "lr=3e-4\n",
    "channel=256\n",
    "n_res_block=4\n",
    "n_res_channel=256\n",
    "n_out_res_block=0\n",
    "n_cond_res_block=3\n",
    "dropout=0.1\n",
    "amp='O0'\n",
    "sched=None\n",
    "ckpt=None\n",
    "path=None\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "dataset = LMDBDataset(path)\n",
    "loader = DataLoader(\n",
    "    dataset, batch_size=batch, shuffle=True, num_workers=4, drop_last=True\n",
    ")\n",
    "\n",
    "if hier == 'top':\n",
    "    model = PixelSNAIL(\n",
    "        [32, 32],\n",
    "        512,\n",
    "        channel,\n",
    "        5,\n",
    "        4,\n",
    "        n_res_block,\n",
    "        n_res_channel,\n",
    "        dropout=dropout,\n",
    "        n_out_res_block=n_out_res_block,\n",
    "    )\n",
    "\n",
    "elif hier == 'bottom':\n",
    "    model = PixelSNAIL(\n",
    "        [64, 64],\n",
    "        512,\n",
    "        channel,\n",
    "        5,\n",
    "        4,\n",
    "        n_res_block,\n",
    "        n_res_channel,\n",
    "        attention=False,\n",
    "        dropout=dropout,\n",
    "        n_cond_res_block=n_cond_res_block,\n",
    "        cond_res_channel=n_res_channel,\n",
    "    )\n",
    "\n",
    "\n",
    "if 'model' in ckpt:\n",
    "    model.load_state_dict(torch.load(ckpt))\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "scheduler = CycleScheduler(\n",
    "    optimizer, lr, n_iter=len(loader) * epoch, momentum=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe720a9-7c21-4e78-8bed-9afb72ebcfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    train( i, loader, model, optimizer, scheduler, device)\n",
    "    torch.save(\n",
    "        {'model': model.module.state_dict(), ': ,\n",
    "        f'checkpoint/pixelsnail_{hier}_{str(i + 1).zfill(3)}.pt',\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
