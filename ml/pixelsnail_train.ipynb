{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6136bab-dbfd-44b3-ac6a-186ac448461a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "\n",
    "except ImportError:\n",
    "    amp = None\n",
    "\n",
    "# from dataset import LMDBDataset\n",
    "from pixelsnail import PixelSNAIL\n",
    "from scheduler import CycleScheduler\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from skimage import io\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# from scheduler import CycleScheduler\n",
    "from pt_utils import  Embeddings, Trainer, VQVAE, data_sampler, Vqvae2Adaptive\n",
    "from torch.utils import data\n",
    "from torch import distributed as dist\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from skimage import transform, metrics\n",
    "import skimage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "from tensordict import TensorDict\n",
    "\n",
    "seed = 51\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f91712-7344-4716-b9a0-2e7b35e67828",
   "metadata": {},
   "source": [
    "# Create embeddings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb41a1ea-d9ce-46e3-926e-ff5e7daff1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_path = '../data/dataset_512/'\n",
    "# dataset_path = '../datasets/bc_right_sub_left_minmax_4x_360'\n",
    "# dataset_path = '../datasets/bc_left_4x_360'\n",
    "dataset_path = '../datasets/original/o_bc_left_9x_512_360'\n",
    "# dataset_path = '../datasets/original/o_bc_left_4x_768'\n",
    "\n",
    "new_dataset_path='../datasets/original/emb_dim_1_n_embed_8192_bc_left_9x_512_360'\n",
    "if os.path.exists(new_dataset_path) is False:\n",
    "    os.mkdir(new_dataset_path)\n",
    "\n",
    "device='cuda'\n",
    "\n",
    "resize_shape = (512, 512)\n",
    "# resize_shape = (1024, 1024)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.Resize(resize_shape),\n",
    "        # transforms.CenterCrop(resize_shape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "model_file = 'data/logs/emb_dim_1_n_embed_8192_bc_left_9x_512_360/vqvae_002_train_0.01976_test_0.01984.pt'\n",
    "\n",
    "model =    VQVAE(in_channel=3,\n",
    "                   channel=128,\n",
    "                   n_res_block=6,\n",
    "                   n_res_channel=32,\n",
    "                   embed_dim=1,\n",
    "                   n_embed=8192,\n",
    "                   decay=0.99).to(device)\n",
    "\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "device='cuda'\n",
    "\n",
    "images_embs_t = []\n",
    "images_embs_b = []\n",
    "\n",
    "dataset_path = dataset.__dict__['root']\n",
    "classes_folders = os.listdir(dataset_path)\n",
    "classes_folders_images = [os.listdir(dataset_path + '/' + folder) for folder in classes_folders]\n",
    "classes_folders_images_num = [len(os.listdir(dataset_path + '/' + folder)) for folder in classes_folders]\n",
    "\n",
    "img_transform = dataset.__dict__['transform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e3f0f-7968-4952-b9f4-c1d044fc91d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensordict = TensorDict(\n",
    "    {\"a\": torch.zeros(1,32,32), \"b\": torch.ones(1, 64, 64)}, batch_size=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948338ae-1de9-494f-b3d9-c3d9348b5827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensordict['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac990d7-6c1f-41df-9425-c53f25b18cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(classes_folders)):\n",
    "    print(f'Number of folders {i + 1}/{len(classes_folders)}')\n",
    "    \n",
    "    os.mkdir(new_dataset_path + '/'+classes_folders[i])\n",
    "        \n",
    "    for j in tqdm(range(classes_folders_images_num[i]), desc=f'Folder {classes_folders[i]}'):\n",
    "        image_path = dataset_path + '/' + classes_folders[i] + '/' + classes_folders_images[i][j]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = img_transform(image)\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        \n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        quant_t, quant_b, _, _, _ = model.encode(image)\n",
    "        \n",
    "        # quant_t.requires_grad=False\n",
    "        # quant_b.requires_grad=False\n",
    "\n",
    "        # torch.save(torch.stack([quant_t, quant_b]), new_dataset_path+ '/'+classes_folders[i]+'/'+classes_folders_images[i][j][:-5]+'.pt',)\n",
    "        torch.save(TensorDict({\"top\": quant_t, \"bottom\": quant_b}, batch_size=[1]),\n",
    "                   new_dataset_path+ '/'+classes_folders[i]+'/'+classes_folders_images[i][j][:-5]+'.pt')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f99bf8-b7c8-4085-9fef-ccdb571d5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import lmdb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import ImageFileDataset, CodeRow\n",
    "from vqvae import VQVAE\n",
    "\n",
    "\n",
    "def extract(lmdb_env, loader, model, device):\n",
    "    index = 0\n",
    "\n",
    "    with lmdb_env.begin(write=True) as txn:\n",
    "        pbar = tqdm(loader)\n",
    "\n",
    "        for img, _, filename in pbar:\n",
    "            img = img.to(device)\n",
    "\n",
    "            _, _, _, id_t, id_b = model.encode(img)\n",
    "            id_t = id_t.detach().cpu().numpy()\n",
    "            id_b = id_b.detach().cpu().numpy()\n",
    "\n",
    "            for file, top, bottom in zip(filename, id_t, id_b):\n",
    "                row = CodeRow(top=top, bottom=bottom, filename=file)\n",
    "                txn.put(str(index).encode('utf-8'), pickle.dumps(row))\n",
    "                index += 1\n",
    "                pbar.set_description(f'inserted: {index}')\n",
    "\n",
    "        txn.put('length'.encode('utf-8'), str(index).encode('utf-8'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--size', type=int, default=256)\n",
    "    parser.add_argument('--ckpt', type=str)\n",
    "    parser.add_argument('--name', type=str)\n",
    "    parser.add_argument('path', type=str)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    device = 'cuda'\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(args.size),\n",
    "            transforms.CenterCrop(args.size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = ImageFileDataset(args.path, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = VQVAE()\n",
    "    model.load_state_dict(torch.load(args.ckpt))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    map_size = 100 * 1024 * 1024 * 1024\n",
    "\n",
    "    env = lmdb.open(args.name, map_size=map_size)\n",
    "\n",
    "    extract(env, loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de832c55-d8f4-48f1-b046-51e01b1663cb",
   "metadata": {},
   "source": [
    "# Train pixelsnail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77306b8-e81f-48ba-84ce-915dada2d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeRow = namedtuple('CodeRow', ['top', 'bottom', 'filename'])\n",
    "\n",
    "\n",
    "# class ImageFileDataset(datasets.ImageFolder):\n",
    "#     def __getitem__(self, index):\n",
    "#         sample, target = super().__getitem__(index)\n",
    "#         path, _ = self.samples[index]\n",
    "#         dirs, filename = os.path.split(path)\n",
    "#         _, class_name = os.path.split(dirs)\n",
    "#         filename = os.path.join(class_name, filename)\n",
    "\n",
    "#         return sample, target, filename\n",
    "\n",
    "\n",
    "class LMDBDataset(datasets.DatasetFolder):\n",
    "#     def __init__(self, path):\n",
    "#         self.env = lmdb.open(\n",
    "#             path,\n",
    "#             max_readers=32,\n",
    "#             readonly=True,\n",
    "#             lock=False,\n",
    "#             readahead=False,\n",
    "#             meminit=False,\n",
    "#         )\n",
    "\n",
    "\n",
    "#         self.length = int(txn.get('length'.encode('utf-8')).decode('utf-8'))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            key = str(index).encode('utf-8')\n",
    "\n",
    "            row = pickle.loads(txn.get(key))\n",
    "\n",
    "        return torch.from_numpy(row.top), torch.from_numpy(row.bottom), row.filename\n",
    "    \n",
    "    \n",
    "class EmbsDataset(torchvision.datasets.DatasetFolder):\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        pt_dict= torch.load(path)\n",
    "        \n",
    "        return pt_dict['top'], pt_dict['bottom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3acdc-b059-4ba1-ae60-f1f62306a521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = '../datasets/original/emb_dim_1_n_embed_8192_bc_left_9x_512_360/'\n",
    "\n",
    "dataset = EmbsDataset(dataset_path, loader=torch.load,extensions=['.pt'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc558fd-bb3a-4263-8631-7f50fe7792e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loader, model, optimizer, scheduler, device):\n",
    "    loader = tqdm(loader)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i, (top, bottom) in tqdm(enumerate(loader)):\n",
    "        model.zero_grad()\n",
    "\n",
    "        top = top.to(device)\n",
    "        bottom = bottom.to(device)\n",
    "        \n",
    "        top=torch.squeeze(top,[1,2])\n",
    "        bottom=torch.squeeze(bottom, [1,2])\n",
    "\n",
    "        if hier == 'top':\n",
    "            target = top\n",
    "            print(top.shape)\n",
    "            print(model(top))\n",
    "            print(model(top).shape)\n",
    "            out, _ = model(top)\n",
    "\n",
    "        elif hier == 'bottom':\n",
    "            bottom = data.to(bottom)\n",
    "            target = bottom\n",
    "            out, _ = model(bottom, condition=top)\n",
    "\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, pred = out.max(1)\n",
    "        correct = (pred == target).float()\n",
    "        accuracy = correct.sum() / target.numel()\n",
    "\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f'Train epoch: {epoch + 1}; loss: {loss.item():.5f}; acc: {accuracy:.5f}; lr: {lr:.5f}')\n",
    "\n",
    "        \n",
    "def evaluate( epoch, loader, model, optimizer, scheduler, device):\n",
    "    loader = tqdm(loader)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for i, (top, bottom) in tqdm(enumerate(loader)):\n",
    "\n",
    "\n",
    "            top = top.to(device)\n",
    "\n",
    "            if args.hier == 'top':\n",
    "                target = top\n",
    "                out, _ = model(top)\n",
    "\n",
    "            elif args.hier == 'bottom':\n",
    "                bottom = data.to(device)\n",
    "                target = bottom\n",
    "                out, _ = model(bottom, condition=top)\n",
    "\n",
    "            loss = criterion(out, target)\n",
    "\n",
    "            _, pred = out.max(1)\n",
    "            correct = (pred == target).float()\n",
    "            accuracy = correct.sum() / target.numel()\n",
    "\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            print(f'Test epoch: {epoch + 1}; loss: {loss.item():.5f}; acc: {accuracy:.5f}; lr: {lr:.5f}')\n",
    "\n",
    "            return round(loss.item()), round(accuracy,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2510f-90f9-4ada-a436-76fef5705203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name='Ultra_Co6_2/Ultra_Co6_2-001_part_2_angle_270.pt'\n",
    "torch.load(dataset_path+'/'+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212fc84-b7b0-4ef9-a84e-cb1d3eae70e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = '../datasets/original/emb_dim_1_n_embed_8192_bc_left_9x_512_360'\n",
    "\n",
    "n_gpu = 1\n",
    "batch_size = 4\n",
    "val_split = 0.15\n",
    "\n",
    "dataset = EmbsDataset(dataset_path, loader=torch.load, extensions=['.pt'])\n",
    "\n",
    "train_dataset_len = int(len(dataset) * (1 - val_split))\n",
    "test_dataset_len = len(dataset) - train_dataset_len\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_dataset_len, test_dataset_len],\n",
    "                                                            generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "train_sampler = data_sampler(train_dataset, shuffle=True, distributed=False)\n",
    "test_sampler = data_sampler(test_dataset, shuffle=True, distributed=False)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size // n_gpu, sampler=train_sampler\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size // n_gpu, sampler=test_sampler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf86b5-b36d-43f0-95de-5ba55518066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=32\n",
    "hier='top'\n",
    "lr=3e-4\n",
    "channel=256\n",
    "n_res_block=4\n",
    "n_res_channel=256\n",
    "n_out_res_block=0\n",
    "n_cond_res_block=3\n",
    "dropout=0.1\n",
    "amp='O0'\n",
    "sched=None\n",
    "ckpt=None\n",
    "path=None\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "if hier == 'top':\n",
    "    model = PixelSNAIL(\n",
    "        shape=[32, 32],\n",
    "        n_class=512,\n",
    "        channel=channel,\n",
    "        kernel_size=5,\n",
    "        n_block=4,\n",
    "        n_res_block=n_res_block,\n",
    "        res_channel=n_res_channel,\n",
    "        dropout=dropout,\n",
    "        n_out_res_block=n_out_res_block,\n",
    "    )\n",
    "\n",
    "elif hier == 'bottom':\n",
    "    model = PixelSNAIL(\n",
    "        shape=[64, 64],\n",
    "        n_class=512,\n",
    "        channel=channel,\n",
    "        kernel_size=5,\n",
    "        n_block=4,\n",
    "        n_res_block=n_res_block,\n",
    "        res_channel=n_res_channel,\n",
    "        attention=False,\n",
    "        dropout=dropout,\n",
    "        n_cond_res_block=n_cond_res_block,\n",
    "        cond_res_channel=n_res_channel,\n",
    "    )\n",
    "\n",
    "\n",
    "# if 'model' in ckpt:\n",
    "#     model.load_state_dict(torch.load(ckpt))\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe720a9-7c21-4e78-8bed-9afb72ebcfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=420\n",
    "device='cuda'\n",
    "\n",
    "scheduler = CycleScheduler(\n",
    "    optimizer, lr, n_iter=len(train_loader) * epoch, momentum=None\n",
    ")\n",
    "\n",
    "for i in range(epoch):\n",
    "    train( i, train_loader, model, optimizer, scheduler, device)\n",
    "    loss, acc = evaluate( i, test_loader, model, optimizer, scheduler, device)\n",
    "    torch.save(model.state_dict(), f'checkpoint/pixelsnail_{hier}_{str(i + 1)}_loss_{loss}_acc_{acc}.pt')\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4e012-4ae2-4f57-895b-3c0ce83d848d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from model import PixelSnail\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "TRY_CUDA = True\n",
    "MODEL_SAVING = True\n",
    "NB_EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if TRY_CUDA and torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"> Using device {device}\")\n",
    "\n",
    "    print(f\"> Instantiating PixelSnail\")\n",
    "    # model = PixelSnail([28, 28], 256, 32, 5, 3, 2, 16, nb_out_res_block=2).to(device)\n",
    "    model = PixelSnail([28, 28], 256, 32, 5, 3, 2, 16, nb_cond_res_block=2, cond_res_channel=16, nb_out_res_block=2).to(device)\n",
    "    print(f\"> Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\\n\")\n",
    "\n",
    "    print(\"> Loading dataset\")\n",
    "    train_dataset = torchvision.datasets.MNIST('data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "    test_dataset = torchvision.datasets.MNIST('data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "\n",
    "    save_id = int(time.time())\n",
    "\n",
    "    for ei in range(NB_EPOCHS):\n",
    "        print(f\"\\n> Epoch {ei+1}/{NB_EPOCHS}\")\n",
    "        train_loss = 0.0\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for x_input, c in tqdm(train_loader):\n",
    "            optim.zero_grad()\n",
    "            x = (x_input*255).long().squeeze().to(device)\n",
    "            c = c.view(-1,1,1).expand(-1,7,7).to(device)\n",
    "\n",
    "            pred, _ = model(x, c=c)\n",
    "            loss = crit(pred.view(BATCH_SIZE, 256, -1), x.view(BATCH_SIZE, -1))\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (x, c) in enumerate(tqdm(test_loader)):\n",
    "                optim.zero_grad()\n",
    "                x = (x*255).long().squeeze().to(device)\n",
    "                c = c.view(-1,1,1).expand(-1,7,7).to(device)\n",
    "\n",
    "                pred, _ = model(x, c=c)\n",
    "                loss = crit(pred.view(BATCH_SIZE, 256, -1), x.view(BATCH_SIZE, -1))\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "                if i == 0:\n",
    "                    img = torch.cat([x, torch.argmax(pred, dim=1)], dim=0) / 255.\n",
    "                    torchvision.utils.save_image(img.unsqueeze(1), f\"imgs/pixelcnn-{ei}.png\")\n",
    "        torch.save(model.state_dict(), f\"checkpoints/{save_id}-{ei}-pixelcnn.pt\")\n",
    "        print(f\"> Training Loss: {train_loss / len(train_loader)}\")\n",
    "        print(f\"> Evaluation Loss: {eval_loss / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc24fcfa-3c42-44da-b11f-f34b98a0b6db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x=x_input\n",
    "x1=(x*255).long().squeeze().to(device)\n",
    "\n",
    "x2 = F.one_hot(x1, 256).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd01984-7381-48b6-b666-5ce2ed40c475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x1[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
