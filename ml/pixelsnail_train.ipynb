{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6136bab-dbfd-44b3-ac6a-186ac448461a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "\n",
    "except ImportError:\n",
    "    amp = None\n",
    "\n",
    "# from dataset import LMDBDataset\n",
    "from pixelsnail import PixelSNAIL\n",
    "from scheduler import CycleScheduler\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from skimage import io\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# from scheduler import CycleScheduler\n",
    "from pt_utils import  Embeddings, Trainer, VQVAE, data_sampler, Vqvae2Adaptive\n",
    "from torch.utils import data\n",
    "from torch import distributed as dist\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from skimage import transform, metrics\n",
    "import skimage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "from tensordict import TensorDict\n",
    "\n",
    "seed = 51\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f91712-7344-4716-b9a0-2e7b35e67828",
   "metadata": {},
   "source": [
    "# Create embeddings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb41a1ea-d9ce-46e3-926e-ff5e7daff1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_path = '../data/dataset_512/'\n",
    "# dataset_path = '../datasets/bc_right_sub_left_minmax_4x_360'\n",
    "# dataset_path = '../datasets/bc_left_4x_360'\n",
    "dataset_path = '../datasets/original/o_bc_left_9x_512_360'\n",
    "# dataset_path = '../datasets/original/o_bc_left_4x_768'\n",
    "\n",
    "new_dataset_path='../datasets/original/emb_dim_1_n_embed_256_bc_left_9x_512_360'\n",
    "if os.path.exists(new_dataset_path) is False:\n",
    "    os.mkdir(new_dataset_path)\n",
    "\n",
    "device='cuda'\n",
    "\n",
    "resize_shape = (512, 512)\n",
    "# resize_shape = (1024, 1024)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.Resize(resize_shape),\n",
    "        # transforms.CenterCrop(resize_shape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "# model_file = 'data/logs/emb_dim_1_n_embed_512_bc_left_9x_512_360/vqvae_010_train_0.01894_test_0.01887.pt'\n",
    "model_file = 'data/logs/emb_dim_1_n_embed_256_bc_left_9x_512_360/vqvae_007_train_0.01952_test_0.01941.pt'\n",
    "\n",
    "model =    VQVAE(in_channel=3,\n",
    "                   channel=128,\n",
    "                   n_res_block=6,\n",
    "                   n_res_channel=32,\n",
    "                   embed_dim=1,\n",
    "                   n_embed=256,\n",
    "                   decay=0.99).to(device)\n",
    "\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "device='cuda'\n",
    "\n",
    "images_embs_t = []\n",
    "images_embs_b = []\n",
    "\n",
    "dataset_path = dataset.__dict__['root']\n",
    "classes_folders = os.listdir(dataset_path)\n",
    "classes_folders_images = [os.listdir(dataset_path + '/' + folder) for folder in classes_folders]\n",
    "classes_folders_images_num = [len(os.listdir(dataset_path + '/' + folder)) for folder in classes_folders]\n",
    "\n",
    "img_transform = dataset.__dict__['transform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac990d7-6c1f-41df-9425-c53f25b18cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(classes_folders)):\n",
    "    print(f'Number of folders {i + 1}/{len(classes_folders)}')\n",
    "    \n",
    "    if os.path.exists(new_dataset_path + '/'+classes_folders[i]) is False:\n",
    "        os.mkdir(new_dataset_path + '/'+classes_folders[i])\n",
    "        \n",
    "    for j in tqdm(range(classes_folders_images_num[i]), desc=f'Folder {classes_folders[i]}'):\n",
    "        image_path = dataset_path + '/' + classes_folders[i] + '/' + classes_folders_images[i][j]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = img_transform(image)\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        \n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        _, _, _, id_t, id_b = model.encode(image)\n",
    "    \n",
    "\n",
    "        # torch.save(torch.stack([quant_t, quant_b]), new_dataset_path+ '/'+classes_folders[i]+'/'+classes_folders_images[i][j][:-5]+'.pt',)\n",
    "        torch.save(TensorDict({\"top\": id_t, \"bottom\": id_b}, batch_size=[1]),\n",
    "                   new_dataset_path+ '/'+classes_folders[i]+'/'+classes_folders_images[i][j][:-5]+'.pt')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de832c55-d8f4-48f1-b046-51e01b1663cb",
   "metadata": {},
   "source": [
    "# Train pixelsnail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc558fd-bb3a-4263-8631-7f50fe7792e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbsDataset(torchvision.datasets.DatasetFolder):\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        pt_dict= torch.load(path)\n",
    "        \n",
    "        return pt_dict['top'], pt_dict['bottom']\n",
    "\n",
    "def train(epoch, loader, model, optimizer, scheduler, device):\n",
    "    loader = tqdm(loader)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i, (top, bottom) in tqdm(enumerate(loader)):\n",
    "        model.zero_grad()\n",
    "\n",
    "        top = top.to(device)\n",
    "        bottom = bottom.to(device)\n",
    "        \n",
    "        top=torch.squeeze(top,[1])\n",
    "        # print(top)\n",
    "        bottom=torch.squeeze(bottom, [1])\n",
    "\n",
    "        if hier == 'top':\n",
    "            target = top\n",
    "            out, _ = model(top)\n",
    "\n",
    "        elif hier == 'bottom':\n",
    "            bottom = data.to(bottom)\n",
    "            target = bottom\n",
    "            out, _ = model(bottom, condition=top)\n",
    "\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, pred = out.max(1)\n",
    "        correct = (pred == target).float()\n",
    "        accuracy = correct.sum() / target.numel()\n",
    "\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f'Train epoch: {epoch + 1}; loss: {loss.item():.5f}; acc: {accuracy:.5f}; lr: {lr:.5f}')\n",
    "\n",
    "        \n",
    "def evaluate( epoch, loader, model, optimizer, scheduler, device):\n",
    "    loader = tqdm(loader)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for i, (top, bottom) in tqdm(enumerate(loader)):\n",
    "\n",
    "            top = top.to(device)\n",
    "            bottom = bottom.to(device)\n",
    "\n",
    "            top=torch.squeeze(top,[1])\n",
    "            bottom=torch.squeeze(bottom, [1])\n",
    "\n",
    "\n",
    "            if args.hier == 'top':\n",
    "                target = top\n",
    "                out, _ = model(top)\n",
    "\n",
    "            elif args.hier == 'bottom':\n",
    "                bottom = data.to(device)\n",
    "                target = bottom\n",
    "                out, _ = model(bottom, condition=top)\n",
    "\n",
    "            loss = criterion(out, target)\n",
    "\n",
    "            _, pred = out.max(1)\n",
    "            correct = (pred == target).float()\n",
    "            accuracy = correct.sum() / target.numel()\n",
    "\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            print(f'Test epoch: {epoch + 1}; loss: {loss.item():.5f}; acc: {accuracy:.5f}; lr: {lr:.5f}')\n",
    "\n",
    "            return round(loss.item()), round(accuracy,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212fc84-b7b0-4ef9-a84e-cb1d3eae70e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = '../datasets/original/emb_dim_1_n_embed_256_bc_left_9x_512_360'\n",
    "\n",
    "\n",
    "n_gpu = 1\n",
    "batch_size = 4\n",
    "val_split = 0.15\n",
    "\n",
    "dataset = EmbsDataset(dataset_path, loader=torch.load, extensions=['.pt'])\n",
    "\n",
    "train_dataset_len = int(len(dataset) * (1 - val_split))\n",
    "test_dataset_len = len(dataset) - train_dataset_len\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_dataset_len, test_dataset_len],\n",
    "                                                            generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "train_sampler = data_sampler(train_dataset, shuffle=True, distributed=False)\n",
    "test_sampler = data_sampler(test_dataset, shuffle=True, distributed=False)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size // n_gpu, sampler=train_sampler\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size // n_gpu, sampler=test_sampler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf86b5-b36d-43f0-95de-5ba55518066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=32\n",
    "hier='top'\n",
    "lr=1e-4\n",
    "channel=256\n",
    "n_res_block=4\n",
    "n_res_channel=256\n",
    "n_out_res_block=0\n",
    "n_cond_res_block=3\n",
    "dropout=0.1\n",
    "amp='O0'\n",
    "sched=None\n",
    "ckpt=None\n",
    "path=None\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "if hier == 'top':\n",
    "    model = PixelSNAIL(\n",
    "        shape=[64, 64],\n",
    "        n_class=256,\n",
    "        channel=channel,\n",
    "        kernel_size=5,\n",
    "        n_block=4,\n",
    "        n_res_block=n_res_block,\n",
    "        res_channel=n_res_channel,\n",
    "        dropout=dropout,\n",
    "        n_out_res_block=n_out_res_block,\n",
    "    )\n",
    "\n",
    "elif hier == 'bottom':\n",
    "    model = PixelSNAIL(\n",
    "        shape=[128, 128],\n",
    "        n_class=256,\n",
    "        channel=channel,\n",
    "        kernel_size=5,\n",
    "        n_block=4,\n",
    "        n_res_block=n_res_block,\n",
    "        res_channel=n_res_channel,\n",
    "        attention=False,\n",
    "        dropout=dropout,\n",
    "        n_cond_res_block=n_cond_res_block,\n",
    "        cond_res_channel=n_res_channel,\n",
    "    )\n",
    "\n",
    "\n",
    "# if 'model' in ckpt:\n",
    "#     model.load_state_dict(torch.load(ckpt))\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bd4483-b443-4c25-96f3-32ca33e0f94e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe720a9-7c21-4e78-8bed-9afb72ebcfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=420\n",
    "device='cuda'\n",
    "\n",
    "scheduler = CycleScheduler(\n",
    "    optimizer, lr, n_iter=len(train_loader) * epoch, momentum=None\n",
    ")\n",
    "\n",
    "for i in range(epoch):\n",
    "    train( i, train_loader, model, optimizer, None, device)\n",
    "    loss, acc = evaluate( i, test_loader, model, optimizer, scheduler, device)\n",
    "    torch.save(model.state_dict(), f'checkpoint/pixelsnail_{hier}_{str(i + 1)}_loss_{loss}_acc_{acc}.pt')\n",
    "         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
