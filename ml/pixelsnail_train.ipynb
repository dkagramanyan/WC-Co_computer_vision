{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6136bab-dbfd-44b3-ac6a-186ac448461a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Администратор\\AppData\\Local\\Temp\\ipykernel_2144\\339495344.py:55: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import gaussian_filter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x226cb00b570>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "\n",
    "except ImportError:\n",
    "    amp = None\n",
    "\n",
    "# from dataset import LMDBDataset\n",
    "from pixelsnail import PixelSNAIL\n",
    "from scheduler import CycleScheduler\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from skimage import io\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# from scheduler import CycleScheduler\n",
    "from pt_utils import  Embeddings, Trainer, VQVAE, data_sampler, Vqvae2Adaptive\n",
    "from torch.utils import data\n",
    "from torch import distributed as dist\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from skimage import transform, metrics\n",
    "import skimage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "seed = 51\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f91712-7344-4716-b9a0-2e7b35e67828",
   "metadata": {},
   "source": [
    "# Create embeddings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb41a1ea-d9ce-46e3-926e-ff5e7daff1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_path = '../data/dataset_512/'\n",
    "# dataset_path = '../datasets/bc_right_sub_left_minmax_4x_360'\n",
    "# dataset_path = '../datasets/bc_left_4x_360'\n",
    "dataset_path = '../datasets/original/o_bc_left_9x_512_360'\n",
    "# dataset_path = '../datasets/original/o_bc_left_4x_768'\n",
    "\n",
    "new_dataset_path='../datasets/original/emb_dim_1_n_embed_8192_bc_left_9x_512_360'\n",
    "if os.path.exists(new_dataset_path) is False:\n",
    "    os.mkdir(new_dataset_path)\n",
    "\n",
    "device='cuda'\n",
    "\n",
    "resize_shape = (512, 512)\n",
    "# resize_shape = (1024, 1024)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.Resize(resize_shape),\n",
    "        # transforms.CenterCrop(resize_shape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "model_file = 'data/logs/emb_dim_1_n_embed_8192_bc_left_9x_512_360/vqvae_002_train_0.01976_test_0.01984.pt'\n",
    "\n",
    "model =    VQVAE(in_channel=3,\n",
    "                   channel=128,\n",
    "                   n_res_block=6,\n",
    "                   n_res_channel=32,\n",
    "                   embed_dim=1,\n",
    "                   n_embed=8192,\n",
    "                   decay=0.99).to(device)\n",
    "\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "device='cuda'\n",
    "\n",
    "images_embs_t = []\n",
    "images_embs_b = []\n",
    "\n",
    "dataset_path = dataset.__dict__['root']\n",
    "classes_folders = os.listdir(dataset_path)\n",
    "classes_folders_images = [os.listdir(dataset_path + '/' + folder) for folder in classes_folders]\n",
    "classes_folders_images_num = [len(os.listdir(dataset_path + '/' + folder)) for folder in classes_folders]\n",
    "\n",
    "img_transform = dataset.__dict__['transform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c477c84-ac60-4efb-83e9-aa2cbfc14b36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ultra_Co11-23_part_6_angle_90'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_folders_images[i][j][:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d483067-4ae0-4685-950d-c578ccc3df11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.mkdir(new_dataset_path + '/top/' )\n",
    "os.mkdir(new_dataset_path + '/bottom/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac990d7-6c1f-41df-9425-c53f25b18cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folders 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463bcfaec5e542c897e155df20759d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Folder Ultra_Co11:   0%|          | 0/3240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folders 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8d5389ee4746af9a46803afaf96e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Folder Ultra_Co15:   0%|          | 0/3240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folders 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c81532364b0449f9a227d28166eddaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Folder Ultra_Co25:   0%|          | 0/3240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folders 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad770d4c7404cc993e8d9d22e4100bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Folder Ultra_Co6_2:   0%|          | 0/3240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folders 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59edd40b986f426784eb17e02f6aa3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Folder Ultra_Co8:   0%|          | 0/3240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(classes_folders)):\n",
    "    print(f'Number of folders {i + 1}/{len(classes_folders)}')\n",
    "    \n",
    "    os.mkdir(new_dataset_path + '/top/' + classes_folders[i])\n",
    "    os.mkdir(new_dataset_path + '/bottom/' + classes_folders[i])\n",
    "        \n",
    "    for j in tqdm(range(classes_folders_images_num[i]), desc=f'Folder {classes_folders[i]}'):\n",
    "        image_path = dataset_path + '/' + classes_folders[i] + '/' + classes_folders_images[i][j]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = img_transform(image)\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        \n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        quant_t, quant_b, _, _, _ = model.encode(image)\n",
    "        \n",
    "        # quant_t.requires_grad=False\n",
    "        # quant_b.requires_grad=False\n",
    "\n",
    "        torch.save(quant_t,new_dataset_path+ '/top/' + classes_folders[i]+'/'+classes_folders_images[i][j][:-5]+'.pt',)\n",
    "        torch.save(quant_b,new_dataset_path+ '/bottom/' + classes_folders[i]+'/'+classes_folders_images[i][j][:-5]+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f99bf8-b7c8-4085-9fef-ccdb571d5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import lmdb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import ImageFileDataset, CodeRow\n",
    "from vqvae import VQVAE\n",
    "\n",
    "\n",
    "def extract(lmdb_env, loader, model, device):\n",
    "    index = 0\n",
    "\n",
    "    with lmdb_env.begin(write=True) as txn:\n",
    "        pbar = tqdm(loader)\n",
    "\n",
    "        for img, _, filename in pbar:\n",
    "            img = img.to(device)\n",
    "\n",
    "            _, _, _, id_t, id_b = model.encode(img)\n",
    "            id_t = id_t.detach().cpu().numpy()\n",
    "            id_b = id_b.detach().cpu().numpy()\n",
    "\n",
    "            for file, top, bottom in zip(filename, id_t, id_b):\n",
    "                row = CodeRow(top=top, bottom=bottom, filename=file)\n",
    "                txn.put(str(index).encode('utf-8'), pickle.dumps(row))\n",
    "                index += 1\n",
    "                pbar.set_description(f'inserted: {index}')\n",
    "\n",
    "        txn.put('length'.encode('utf-8'), str(index).encode('utf-8'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--size', type=int, default=256)\n",
    "    parser.add_argument('--ckpt', type=str)\n",
    "    parser.add_argument('--name', type=str)\n",
    "    parser.add_argument('path', type=str)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    device = 'cuda'\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(args.size),\n",
    "            transforms.CenterCrop(args.size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = ImageFileDataset(args.path, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = VQVAE()\n",
    "    model.load_state_dict(torch.load(args.ckpt))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    map_size = 100 * 1024 * 1024 * 1024\n",
    "\n",
    "    env = lmdb.open(args.name, map_size=map_size)\n",
    "\n",
    "    extract(env, loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de832c55-d8f4-48f1-b046-51e01b1663cb",
   "metadata": {},
   "source": [
    "# Train pixelsnail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77306b8-e81f-48ba-84ce-915dada2d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeRow = namedtuple('CodeRow', ['top', 'bottom', 'filename'])\n",
    "\n",
    "\n",
    "# class ImageFileDataset(datasets.ImageFolder):\n",
    "#     def __getitem__(self, index):\n",
    "#         sample, target = super().__getitem__(index)\n",
    "#         path, _ = self.samples[index]\n",
    "#         dirs, filename = os.path.split(path)\n",
    "#         _, class_name = os.path.split(dirs)\n",
    "#         filename = os.path.join(class_name, filename)\n",
    "\n",
    "#         return sample, target, filename\n",
    "\n",
    "\n",
    "class LMDBDataset(datasets.DatasetFolder):\n",
    "#     def __init__(self, path):\n",
    "#         self.env = lmdb.open(\n",
    "#             path,\n",
    "#             max_readers=32,\n",
    "#             readonly=True,\n",
    "#             lock=False,\n",
    "#             readahead=False,\n",
    "#             meminit=False,\n",
    "#         )\n",
    "\n",
    "\n",
    "#         self.length = int(txn.get('length'.encode('utf-8')).decode('utf-8'))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            key = str(index).encode('utf-8')\n",
    "\n",
    "            row = pickle.loads(txn.get(key))\n",
    "\n",
    "        return torch.from_numpy(row.top), torch.from_numpy(row.bottom), row.filename\n",
    "    \n",
    "    \n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root: str, folder: str, klass: int, extension: str = \"png\"):\n",
    "        self._data = pathlib.Path(root) / folder\n",
    "        self.klass = klass\n",
    "        self.extension = extension\n",
    "        # Only calculate once how many files are in this folder\n",
    "        # Could be passed as argument if you precalculate it somehow\n",
    "        # e.g. ls | wc -l on Linux\n",
    "        self._length = sum(1 for entry in os.listdir(self._data))\n",
    "\n",
    "    def __len__(self):\n",
    "        # No need to recalculate this value every time\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # images always follow [0, n-1], so you access them directly\n",
    "        return Image.open(self._data / \"{}.{}\".format(str(index), self.extension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebc558fd-bb3a-4263-8631-7f50fe7792e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loader, model, optimizer, scheduler, device):\n",
    "    loader = tqdm(loader)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i, data in tqdm(enumerate(loader)):\n",
    "        model.zero_grad()\n",
    "\n",
    "        top = top.to(device)\n",
    "\n",
    "        if args.hier == 'top':\n",
    "            target = data\n",
    "            out, _ = model(data)\n",
    "\n",
    "        elif args.hier == 'bottom':\n",
    "            bottom = data.to(device)\n",
    "            target = bottom\n",
    "            out, _ = model(data, condition=top)\n",
    "\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, pred = out.max(1)\n",
    "        correct = (pred == target).float()\n",
    "        accuracy = correct.sum() / target.numel()\n",
    "\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f'Train epoch: {epoch + 1}; loss: {loss.item():.5f}; acc: {accuracy:.5f}; lr: {lr:.5f}')\n",
    "\n",
    "        \n",
    "def evaluate( epoch, loader, model, optimizer, scheduler, device):\n",
    "    loader = tqdm(loader)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(loader)):\n",
    "\n",
    "\n",
    "            top = top.to(device)\n",
    "\n",
    "            if args.hier == 'top':\n",
    "                target = data\n",
    "                out, _ = model(data)\n",
    "\n",
    "            elif args.hier == 'bottom':\n",
    "                bottom = data.to(device)\n",
    "                target = bottom\n",
    "                out, _ = model(data, condition=top)\n",
    "\n",
    "            loss = criterion(out, target)\n",
    "\n",
    "            _, pred = out.max(1)\n",
    "            correct = (pred == target).float()\n",
    "            accuracy = correct.sum() / target.numel()\n",
    "\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            print(f'Test epoch: {epoch + 1}; loss: {loss.item():.5f}; acc: {accuracy:.5f}; lr: {lr:.5f}')\n",
    "\n",
    "            return round(loss.item()), round(accuracy,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02a2510f-90f9-4ada-a436-76fef5705203",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0748, -0.0769, -0.0748,  ..., -0.0769, -0.0637, -0.0996],\n",
       "          [-0.0912, -0.0969, -0.0934,  ..., -0.0946, -0.0922, -0.1024],\n",
       "          [-0.0851, -0.0911, -0.0877,  ..., -0.0889, -0.0875, -0.1064],\n",
       "          ...,\n",
       "          [-0.0859, -0.0932, -0.0896,  ..., -0.0913, -0.0873, -0.1062],\n",
       "          [-0.0806, -0.0932, -0.0896,  ..., -0.0899, -0.0902, -0.1067],\n",
       "          [-0.0637, -0.0851, -0.0822,  ..., -0.0806, -0.0888, -0.0947]]]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name='Ultra_Co6_2/Ultra_Co6_2-001_part_2_angle_270.pt'\n",
    "torch.load(dataset_path+'/'+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9212fc84-b7b0-4ef9-a84e-cb1d3eae70e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = '../datasets/original/emb_dim_1_n_embed_8192_bc_left_9x_512_360/top'\n",
    "\n",
    "n_gpu = 1\n",
    "batch_size = 4\n",
    "val_split = 0.15\n",
    "\n",
    "dataset = datasets.DatasetFolder(dataset_path, loader=torch.load, extensions=['.pt'])\n",
    "\n",
    "train_dataset_len = int(len(dataset) * (1 - val_split))\n",
    "test_dataset_len = len(dataset) - train_dataset_len\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_dataset_len, test_dataset_len],\n",
    "                                                            generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "train_sampler = data_sampler(train_dataset, shuffle=True, distributed=False)\n",
    "test_sampler = data_sampler(test_dataset, shuffle=True, distributed=False)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size // n_gpu, sampler=train_sampler\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size // n_gpu, sampler=test_sampler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bddf86b5-b36d-43f0-95de-5ba55518066e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "batch=32\n",
    "hier='top'\n",
    "lr=3e-4\n",
    "channel=256\n",
    "n_res_block=4\n",
    "n_res_channel=256\n",
    "n_out_res_block=0\n",
    "n_cond_res_block=3\n",
    "dropout=0.1\n",
    "amp='O0'\n",
    "sched=None\n",
    "ckpt=None\n",
    "path=None\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "if hier == 'top':\n",
    "    model = PixelSNAIL(\n",
    "        [32, 32],\n",
    "        512,\n",
    "        channel,\n",
    "        5,\n",
    "        4,\n",
    "        n_res_block,\n",
    "        n_res_channel,\n",
    "        dropout=dropout,\n",
    "        n_out_res_block=n_out_res_block,\n",
    "    )\n",
    "\n",
    "elif hier == 'bottom':\n",
    "    model = PixelSNAIL(\n",
    "        [64, 64],\n",
    "        512,\n",
    "        channel,\n",
    "        5,\n",
    "        4,\n",
    "        n_res_block,\n",
    "        n_res_channel,\n",
    "        attention=False,\n",
    "        dropout=dropout,\n",
    "        n_cond_res_block=n_cond_res_block,\n",
    "        cond_res_channel=n_res_channel,\n",
    "    )\n",
    "\n",
    "\n",
    "# if 'model' in ckpt:\n",
    "#     model.load_state_dict(torch.load(ckpt))\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = CycleScheduler(\n",
    "    optimizer, lr, n_iter=len(train_loader) * epoch, momentum=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfe720a9-7c21-4e78-8bed-9afb72ebcfd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5b1d2d82fd4a0183be87cbcc8f0fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3443 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b1a7c495e54f36ad96252c237b4988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     loss, acc \u001b[38;5;241m=\u001b[39m evaluate( i, test_loader, model, optimizer, scheduler, device)\n\u001b[0;32m      7\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint/pixelsnail_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_loss_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_acc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch, loader, model, optimizer, scheduler, device)\u001b[0m\n\u001b[0;32m      2\u001b[0m loader \u001b[38;5;241m=\u001b[39m tqdm(loader)\n\u001b[0;32m      4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (top, bottom, label) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(loader)):\n\u001b[0;32m      7\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      9\u001b[0m     top \u001b[38;5;241m=\u001b[39m top\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "epoch=420\n",
    "device='cuda'\n",
    "\n",
    "for i in range(epoch):\n",
    "    train( i, train_loader, model, optimizer, scheduler, device)\n",
    "    loss, acc = evaluate( i, test_loader, model, optimizer, scheduler, device)\n",
    "    torch.save(model.state_dict(), f'checkpoint/pixelsnail_{hier}_{str(i + 1)}_loss_{loss}_acc_{acc}.pt')\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4e012-4ae2-4f57-895b-3c0ce83d848d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
