{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T20:47:35.874424600Z",
     "start_time": "2023-10-10T20:47:29.834572300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "import torchvision\n",
    "\n",
    "from pt_utils import  Embeddings, Trainer, VQVAE, data_sampler, Vqvae2AdaptiveVae, VanillaVAE\n",
    "from torchsummary import summary\n",
    "import os\n",
    "from torch import distributed as dist\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from typing import Any, Callable, cast, Dict, List, Optional, Tuple\n",
    "\n",
    "seed = 51\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T21:23:15.387589100Z",
     "start_time": "2023-10-10T21:23:15.308919300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device = \"cuda\"\n",
    "device = \"cpu\"\n",
    "\n",
    "model =    VQVAE(in_channel=3,\n",
    "# model =    Vqvae2Adaptive(in_channel=3,\n",
    "                   channel=128,\n",
    "                   n_res_block=6,\n",
    "                   n_res_channel=32,\n",
    "                   embed_dim=8,\n",
    "                   n_embed=8192,\n",
    "                   decay=0.99).to(device)\n",
    "\n",
    "# model =    Vqvae2Adaptive(in_channel=3,\n",
    "#                  channel=128,\n",
    "#                  n_res_block=6,\n",
    "#                  n_res_channel=32,\n",
    "#                  embed_dim=1,\n",
    "#                  n_embed=8192,\n",
    "#                  decay=0.99).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        l.append((i, name))\n",
    "l[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary(model, input_size=(3, 512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T21:23:18.645361300Z",
     "start_time": "2023-10-10T21:23:18.554540700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_path = '../data/dataset_512/'\n",
    "# dataset_path = '../datasets/bc_right_sub_left_minmax_4x_360'\n",
    "# dataset_path = '../datasets/bc_left_4x_360'\n",
    "# dataset_path = '../datasets/original/o_bc_left_9x_512_360'\n",
    "dataset_path = '../datasets/original/o_bc_left_4x_768'\n",
    "\n",
    "resize_shape = (512, 512)\n",
    "# resize_shape = (1024, 1024)\n",
    "\n",
    "n_gpu = 1\n",
    "batch_size = 4\n",
    "val_split = 0.15\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.Resize(resize_shape),\n",
    "        # transforms.CenterCrop(resize_shape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "\n",
    "train_dataset_len = int(len(dataset) * (1 - val_split))\n",
    "test_dataset_len = len(dataset) - train_dataset_len\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_dataset_len, test_dataset_len],\n",
    "                                                            generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "train_sampler = data_sampler(train_dataset, shuffle=True, distributed=False)\n",
    "test_sampler = data_sampler(test_dataset, shuffle=True, distributed=False)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size // n_gpu, sampler=train_sampler, num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size // n_gpu, sampler=test_sampler, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model_file = 'data/logs/emb_dim_1_n_embed_8192_bc_left_sub_right_minmax_4x_360/vqvae_001_train_0.04914_test_0.04206.pt'\n",
    "model_file = 'data/logs/emb_dim_1_n_embed_8192_bc_right_sub_left_minmax_4x_360/vqvae_003_train_0.04287_test_0.04129.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(model_file, map_location=torch.device('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T21:39:43.538277400Z",
     "start_time": "2023-10-10T21:23:26.983667600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 1e-4\n",
    "\n",
    "latent_loss_weight = 0.25\n",
    "sample_size = 25\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-7, amsgrad=True)\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=lr,weight_decay=1e-6,centered=True)\n",
    "\n",
    "# sample_path = '../data/logs/vq-vae-2/4x/samples'\n",
    "# model_path = '../data/logs/vq-vae-2/4x/weights'\n",
    "\n",
    "# model_path = 'data/logs/emb_dim_1_n_embed_8192_bc_right_sub_left_minmax_4x_360'\n",
    "model_path = 'data/logs/emb_dim_2_n_embed_8192_bc_left_9x_512_360'\n",
    "\n",
    "Trainer.train(model=model, optimizer=optimizer, train_loader=train_loader, test_loader=test_loader,\n",
    "              model_path=model_path, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_path = '../data/dataset_512/'\n",
    "# dataset_path = '../datasets/bc_right_sub_left_minmax_4x_360'\n",
    "# dataset_path = '../datasets/bc_left_4x_360'\n",
    "# dataset_path = '../datasets/original/o_bc_left_9x_512_360'\n",
    "dataset_path = '../datasets/original/o_bc_left_4x_768'\n",
    "\n",
    "resize_shape = (512, 512)\n",
    "# resize_shape = (1024, 1024)\n",
    "\n",
    "n_gpu = 1\n",
    "batch_size = 4\n",
    "val_split = 0.15\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(resize_shape),\n",
    "        # transforms.CenterCrop(resize_shape),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Grayscale(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = TripletFolder(dataset_path, transform=transform)\n",
    "# dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "\n",
    "train_dataset_len = int(len(dataset) * (1 - val_split))\n",
    "test_dataset_len = len(dataset) - train_dataset_len\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_dataset_len, test_dataset_len],\n",
    "                                                            generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "train_sampler = data_sampler(train_dataset, shuffle=True, distributed=False)\n",
    "test_sampler = data_sampler(test_dataset, shuffle=True, distributed=False)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size // n_gpu, sampler=train_sampler, num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size // n_gpu, sampler=test_sampler, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin = 1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def calc_euclidean(self, x1, x2):\n",
    "        return(x1 - x2).pow(2).sum(1)\n",
    "\n",
    "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
    "        distance_positive = self.calc_euclidean(anchor, positive)\n",
    "        distance_negative_a = self.calc_euclidean(anchor, negative)\n",
    "        distance_negative_b = self.calc_euclidean(positive, negative)\n",
    "\n",
    "        losses = torch.relu(distance_positive - (distance_negative_a + distance_negative_b)/2.0 + self.margin)\n",
    "\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_primary():\n",
    "    return get_rank() == 0\n",
    "\n",
    "def get_rank():\n",
    "    if not dist.is_available():\n",
    "        return 0\n",
    "\n",
    "    if not dist.is_initialized():\n",
    "        return 0\n",
    "\n",
    "    return dist.get_rank()\n",
    "\n",
    "def all_gather(data):\n",
    "    world_size = get_world_size()\n",
    "\n",
    "    if world_size == 1:\n",
    "        return [data]\n",
    "\n",
    "    buffer = pickle.dumps(data)\n",
    "    storage = torch.ByteStorage.from_buffer(buffer)\n",
    "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
    "\n",
    "    local_size = torch.IntTensor([tensor.numel()]).to(\"cuda\")\n",
    "    size_list = [torch.IntTensor([1]).to(\"cuda\") for _ in range(world_size)]\n",
    "    dist.all_gather(size_list, local_size)\n",
    "    size_list = [int(size.item()) for size in size_list]\n",
    "    max_size = max(size_list)\n",
    "\n",
    "    tensor_list = []\n",
    "    for _ in size_list:\n",
    "        tensor_list.append(torch.ByteTensor(size=(max_size,)).to(\"cuda\"))\n",
    "\n",
    "    if local_size != max_size:\n",
    "        padding = torch.ByteTensor(size=(max_size - local_size,)).to(\"cuda\")\n",
    "        tensor = torch.cat((tensor, padding), 0)\n",
    "\n",
    "    dist.all_gather(tensor_list, tensor)\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for size, tensor in zip(size_list, tensor_list):\n",
    "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
    "        data_list.append(pickle.loads(buffer))\n",
    "\n",
    "    return data_list\n",
    "\n",
    "def get_world_size():\n",
    "    if not dist.is_available():\n",
    "        return 1\n",
    "\n",
    "    if not dist.is_initialized():\n",
    "        return 1\n",
    "\n",
    "    return dist.get_world_size()\n",
    "\n",
    "def train_triplet(model, optimizer, train_loader, test_loader, model_path, epochs=100, device='cuda',\n",
    "          latent_loss_weight=0.25, sample_size=25):\n",
    "\n",
    "    if os.path.exists(model_path) is False:\n",
    "        os.mkdir(model_path)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        if is_primary():\n",
    "            train_loader = tqdm(train_loader)\n",
    "\n",
    "        criterion = nn.TripletMarginLoss()\n",
    "\n",
    "        mse_sum = 0\n",
    "        mse_n = 0\n",
    "        test_mean_loss = []\n",
    "        train_mean_loss = []\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            model.zero_grad()\n",
    "            anchor_img, positive_img, negative_img = data\n",
    "            \n",
    "            anchor_img = anchor_img.to(device)\n",
    "            positive_img = positive_img.to(device)\n",
    "            negative_img = negative_img.to(device)\n",
    "\n",
    "            anchor_out, anchor_latent_out = model(anchor_img)\n",
    "            positive_out, positive_latent_out = model(positive_img)\n",
    "            negative_out, negative_latent_out = model(negative_img)\n",
    "\n",
    "            # triplet loss \n",
    "            recon_loss = criterion(anchor_out, positive_out, negative_out)\n",
    "            latent_loss=torch.mean(torch.stack([anchor_latent_out.mean(),\n",
    "                                                                 anchor_latent_out.mean(),\n",
    "                                                                 anchor_latent_out.mean() ]))\n",
    "            \n",
    "            loss = recon_loss + latent_loss_weight * latent_loss\n",
    "            loss.backward()\n",
    "            train_mean_loss.append(loss.item())\n",
    "            optimizer.step()\n",
    "\n",
    "            part_mse_sum = recon_loss.item() * anchor_img.shape[0]\n",
    "            part_mse_n = anchor_img.shape[0]\n",
    "            comm = {\"mse_sum\": part_mse_sum, \"mse_n\": part_mse_n}\n",
    "            comm = all_gather(comm)\n",
    "\n",
    "            for part in comm:\n",
    "                mse_sum += part[\"mse_sum\"]\n",
    "                mse_n += part[\"mse_n\"]\n",
    "\n",
    "            if is_primary():\n",
    "                lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "                train_loader.set_description(\n",
    "                    (\n",
    "                        f\"epoch: {epoch + 1}; loss: {str(round(np.mean(train_mean_loss), 5))}; mse: {recon_loss.item():.5f}; \"\n",
    "                        f\"latent: {latent_loss.item():.3f}; avg mse: {mse_sum / mse_n:.5f}; \"\n",
    "                        f\"lr: {lr:.5f}\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for j, data in enumerate(test_loader):\n",
    "                anchor_img, positive_img, negative_img = data\n",
    "\n",
    "                anchor_img = anchor_img.to(device)\n",
    "                positive_img = positive_img.to(device)\n",
    "                negative_img = negative_img.to(device)\n",
    "\n",
    "                anchor_out, anchor_latent_out = model(anchor_img)\n",
    "                positive_out, positive_latent_out = model(positive_img)\n",
    "                negative_out, negative_latent_out = model(negative_img)\n",
    "\n",
    "                # triplet loss \n",
    "                recon_loss = criterion(anchor_out, positive_out, negative_out)\n",
    "\n",
    "                test_loss = recon_loss + latent_loss_weight * torch.mean(torch.stack([anchor_latent_out.mean(),\n",
    "                                                                     anchor_latent_out.mean(),\n",
    "                                                                     anchor_latent_out.mean() ]))\n",
    "                test_mean_loss.append(round(test_loss.item(), 5))\n",
    "\n",
    "            sample = anchor_img[:sample_size]\n",
    "\n",
    "        utils.save_image(\n",
    "            torch.cat([sample, anchor_out], 0),\n",
    "            f\"{model_path}/{str(epoch + 1).zfill(5)}.png\",\n",
    "            nrow=sample_size,\n",
    "            normalize=True,\n",
    "            # range=(-1, 1),\n",
    "        )\n",
    "\n",
    "        print(f'test elbo: {str(round(np.mean(test_mean_loss), 5))}')\n",
    "        torch.save(model.state_dict(),\n",
    "                   f\"{model_path}/vqvae_{str(epoch + 1).zfill(3)}_train_{str(round(np.mean(train_mean_loss), 5))}_test_{str(round(np.mean(test_mean_loss), 5))}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_file = 'data/logs/emb_dim_1_n_embed_8192_bc_left_sub_right_minmax_4x_360/vqvae_001_train_0.04914_test_0.04206.pt'\n",
    "model_file = 'data/logs/emb_dim_1_n_embed_8192_bc_right_sub_left_minmax_4x_360/vqvae_003_train_0.04287_test_0.04129.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(model_file, map_location=torch.device('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 1e-4\n",
    "\n",
    "latent_loss_weight = 0.25\n",
    "sample_size = 25\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-7, amsgrad=True)\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=lr,weight_decay=1e-6,centered=True)\n",
    "\n",
    "# sample_path = '../data/logs/vq-vae-2/4x/samples'\n",
    "# model_path = '../data/logs/vq-vae-2/4x/weights'\n",
    "\n",
    "# model_path = 'data/logs/emb_dim_1_n_embed_8192_bc_right_sub_left_minmax_4x_360'\n",
    "# model_path = 'data/logs/emb_dim_2_n_embed_8192_bc_left_9x_512_360'\n",
    "model_path = 'data/logs/emb_dim_1_n_embed_8192o_o_bc_left_4x_512_triplet'\n",
    "\n",
    "train_triplet(model=model, optimizer=optimizer, train_loader=train_loader, test_loader=test_loader,\n",
    "              model_path=model_path, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ-VAE-2 + VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_path = '../data/dataset_512/'\n",
    "# dataset_path = '../datasets/bc_right_sub_left_minmax_4x_360'\n",
    "# dataset_path = '../datasets/bc_left_4x_360'\n",
    "# dataset_path = '../datasets/original/o_bc_left_9x_512_360'\n",
    "dataset_path = '../datasets/original/o_bc_left_4x_768'\n",
    "\n",
    "resize_shape = (512, 512)\n",
    "# resize_shape = (1024, 1024)\n",
    "\n",
    "n_gpu = 1\n",
    "batch_size =4\n",
    "val_split = 0.15\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(resize_shape),\n",
    "        # transforms.CenterCrop(resize_shape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "\n",
    "train_dataset_len = int(len(dataset) * (1 - val_split))\n",
    "test_dataset_len = len(dataset) - train_dataset_len\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_dataset_len, test_dataset_len],\n",
    "                                                            generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "train_sampler = data_sampler(train_dataset, shuffle=True, distributed=False)\n",
    "test_sampler = data_sampler(test_dataset, shuffle=True, distributed=False)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size // n_gpu, sampler=train_sampler, num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size // n_gpu, sampler=test_sampler, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Vqvae2AdaptiveVae1(Vqvae2AdaptiveVae):\n",
    "\n",
    "    def forward(self, input,n_embedded_l=None, dim_l=None):\n",
    "        quant_t, quant_b, diff, _, _ = self.encode(input,n_embedded_l=n_embedded_l, dim_l=dim_l)\n",
    "        # quant_t=torch.squeeze(quant_t, 1)\n",
    "        # quant_b=torch.squeeze(quant_b, 1)\n",
    "        quant_t_out, mu_t, log_var_t=self.vae_top(quant_t)\n",
    "        quant_b_out, mu_b, log_var_b=self.vae_bottom(quant_b)\n",
    "        dec = self.decode(quant_t_out, quant_b_out)\n",
    "\n",
    "        elbo_t=self.elbo_loss(quant_t_out, quant_t, mu_t, log_var_t)\n",
    "        elbo_b=self.elbo_loss(quant_b_out, quant_b, mu_b, log_var_b)\n",
    "\n",
    "        return dec, diff, elbo_t, elbo_b\n",
    "\n",
    "    def elbo_loss(self, recon_x, x, mu, logvar, beta=1):\n",
    "        \"\"\"\n",
    "        ELBO Optimization objective for gaussian posterior\n",
    "        (reconstruction term + regularization term)\n",
    "        \"\"\"\n",
    "        reconstruction_function = nn.MSELoss(reduction='sum')\n",
    "        MSE = reconstruction_function(recon_x, x)\n",
    "    \n",
    "        # https://arxiv.org/abs/1312.6114 (Appendix B)\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    \n",
    "        KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "        KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    \n",
    "        return MSE + beta*KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "\n",
    "model =    Vqvae2AdaptiveVae1(in_channel=3,\n",
    "                   channel=128,\n",
    "                   n_res_block=6,\n",
    "                   n_res_channel=32,\n",
    "                   embed_dim=1,\n",
    "                   n_embed=8192,\n",
    "                   decay=0.99,\n",
    "                    latent_dims=200).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        l.append((i, name))\n",
    "l[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, test_loader, model_path, epochs=100, device='cuda',\n",
    "          latent_loss_weight=0.25, sample_size=25):\n",
    "\n",
    "    if os.path.exists(model_path) is False:\n",
    "        os.mkdir(model_path)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        if is_primary():\n",
    "            train_loader = tqdm(train_loader)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        mse_sum = 0\n",
    "        mse_n = 0\n",
    "        test_mean_loss = []\n",
    "        train_mean_loss = []\n",
    "\n",
    "        for i, (img, label) in enumerate(train_loader):\n",
    "            model.zero_grad()\n",
    "\n",
    "            img = img.to(device)\n",
    "\n",
    "            out, latent_loss, elbo_t, elbo_b=model(img)\n",
    "            \n",
    "            recon_loss = criterion(out, img)\n",
    "            latent_loss = latent_loss.mean()\n",
    "            loss = recon_loss + latent_loss_weight * latent_loss+elbo_t+elbo_b\n",
    "            loss.backward()\n",
    "            train_mean_loss.append(loss.item())\n",
    "            # if scheduler is not None:\n",
    "            #     scheduler.step()\n",
    "            optimizer.step()\n",
    "\n",
    "            part_mse_sum = recon_loss.item() * img.shape[0]\n",
    "            part_mse_n = img.shape[0]\n",
    "            comm = {\"mse_sum\": part_mse_sum, \"mse_n\": part_mse_n}\n",
    "            comm = all_gather(comm)\n",
    "\n",
    "            for part in comm:\n",
    "                mse_sum += part[\"mse_sum\"]\n",
    "                mse_n += part[\"mse_n\"]\n",
    "\n",
    "            if is_primary():\n",
    "                lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "                train_loader.set_description(\n",
    "                    (\n",
    "                        f\"epoch: {epoch + 1}; loss: {str(round(np.mean(train_mean_loss), 5))}; mse: {recon_loss.item():.5f}; \"\n",
    "                        f\"latent: {latent_loss.item():.3f}; avg mse: {mse_sum / mse_n:.5f}; \"\n",
    "                        f\"lr: {lr:.5f}\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for j, (img, label) in enumerate(test_loader):\n",
    "                img = img.to(device)\n",
    "                out, latent_loss, elbo_t, elbo_b=model(img)\n",
    "                test_recon_loss = criterion(out, img)\n",
    "                test_latent_loss = latent_loss.mean()\n",
    "                test_loss = test_recon_loss + latent_loss_weight * latent_loss+elbo_t+elbo_b\n",
    "                test_mean_loss.append(round(test_loss.item(), 5))\n",
    "\n",
    "            sample = img[:sample_size]\n",
    "\n",
    "        utils.save_image(\n",
    "            torch.cat([sample, out], 0),\n",
    "            f\"{model_path}/{str(epoch + 1).zfill(5)}.png\",\n",
    "            nrow=sample_size,\n",
    "            normalize=True,\n",
    "            # range=(-1, 1),\n",
    "        )\n",
    "\n",
    "        print(f'test elbo: {str(round(np.mean(test_mean_loss), 5))}')\n",
    "        torch.save(model.state_dict(),\n",
    "                   f\"{model_path}/vqvae_{str(epoch + 1).zfill(3)}_train_{str(round(np.mean(train_mean_loss), 5))}_test_{str(round(np.mean(test_mean_loss), 5))}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 1e-4\n",
    "\n",
    "latent_loss_weight = 0.25\n",
    "sample_size = 25\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-7, amsgrad=True)\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=lr,weight_decay=1e-6,centered=True)\n",
    "\n",
    "# sample_path = '../data/logs/vq-vae-2/4x/samples'\n",
    "# model_path = '../data/logs/vq-vae-2/4x/weights'\n",
    "\n",
    "# model_path = 'data/logs/emb_dim_1_n_embed_8192_bc_right_sub_left_minmax_4x_360'\n",
    "# model_path = 'data/logs/emb_dim_1_n_embed_8192_bc_left_9x_512_360'\n",
    "\n",
    "model_path='test'\n",
    "\n",
    "train(model=model, optimizer=optimizer, train_loader=train_loader, test_loader=test_loader,\n",
    "              model_path=model_path, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=VanillaVAE(1,200,flag_128=True, hidden_dims=[16, 32, 64, 128, 256, 512])\n",
    "model.to('cuda')\n",
    "\n",
    "# summary(model, (1, 64, 64))\n",
    "summary(model, (1, 128, 128))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
