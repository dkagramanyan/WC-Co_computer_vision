{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909e6c890ed3e7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T12:00:19.180991200Z",
     "start_time": "2023-10-13T12:00:19.155062400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from skimage import io\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from scheduler import CycleScheduler\n",
    "from pt_utils import  Embeddings, Trainer, VQVAE, data_sampler\n",
    "from torch.utils import data\n",
    "from torch import distributed as dist\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from skimage import transform, metrics\n",
    "import skimage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "# from tqdm.notebook import trange, tqdm\n",
    "from tqdm import trange, tqdm\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from pt_utils import Encoder, Decoder, all_reduce, Quantize\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e9067e6629cb76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T13:44:49.251095900Z",
     "start_time": "2023-10-13T13:44:49.245304900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QuantizeAdaptive(nn.Module):\n",
    "    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.n_embed = n_embed\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "\n",
    "        embed = torch.randn(dim, n_embed)\n",
    "        self.register_buffer(\"embed\", embed)\n",
    "        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n",
    "        self.register_buffer(\"embed_avg\", embed.clone())\n",
    "\n",
    "    def forward(self, input, n_embedded_l=None, dim_l=None):\n",
    "        \n",
    "        flatten = input.reshape(-1, self.dim)\n",
    "        dist = (\n",
    "                flatten.pow(2).sum(1, keepdim=True)\n",
    "                - 2 * flatten @ self.embed[:,:n_embedded_l]\n",
    "                + self.embed[:,:n_embedded_l].pow(2).sum(0, keepdim=True)\n",
    "        )\n",
    "        _, embed_ind = (-dist).max(1)\n",
    "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
    "        embed_ind = embed_ind.view(*input.shape[:-1])\n",
    "        quantize = self.embed_code(embed_ind)\n",
    "\n",
    "        if self.training:\n",
    "            embed_onehot_sum = embed_onehot.sum(0)\n",
    "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
    "\n",
    "            all_reduce(embed_onehot_sum)\n",
    "            all_reduce(embed_sum)\n",
    "\n",
    "            self.cluster_size.data.mul_(self.decay).add_(\n",
    "                embed_onehot_sum, alpha=1 - self.decay\n",
    "            )\n",
    "            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n",
    "            n = self.cluster_size.sum()\n",
    "            cluster_size = (\n",
    "                    (self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n\n",
    "            )\n",
    "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "\n",
    "        diff = (quantize.detach() - input).pow(2).mean()\n",
    "        quantize = input + (quantize - input).detach()\n",
    "        \n",
    "        if dim_l!=None:\n",
    "            quantize[:,:,:,dim_l:]=0\n",
    "        \n",
    "        # return quantize[:,:,:,:dim_l], diff, embed_ind\n",
    "        return quantize, diff, embed_ind\n",
    "\n",
    "    def embed_code(self, embed_id):\n",
    "        return F.embedding(embed_id, self.embed.transpose(0, 1))\n",
    "    \n",
    "class Vqvae2Adaptive1(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channel=3,\n",
    "            channel=128,\n",
    "            n_res_block=5,\n",
    "            n_res_channel=32,\n",
    "            embed_dim=64,\n",
    "            n_embed=512,\n",
    "            decay=0.99,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_b = Encoder(in_channel, channel, n_res_block, n_res_channel, stride=4)\n",
    "        self.enc_t = Encoder(channel, channel, n_res_block, n_res_channel, stride=2)\n",
    "        self.quantize_conv_t = nn.Conv2d(channel, embed_dim, 1)\n",
    "        self.quantize_t = QuantizeAdaptive(embed_dim, n_embed)\n",
    "        self.dec_t = Decoder(\n",
    "            embed_dim, embed_dim, channel, n_res_block, n_res_channel, stride=2\n",
    "        )\n",
    "        self.quantize_conv_b = nn.Conv2d(embed_dim + channel, embed_dim, 1)\n",
    "        self.quantize_b = QuantizeAdaptive(embed_dim, n_embed)\n",
    "        self.upsample_t = nn.ConvTranspose2d(\n",
    "            embed_dim, embed_dim, 4, stride=2, padding=1\n",
    "        )\n",
    "        self.dec = Decoder(\n",
    "            embed_dim + embed_dim,\n",
    "            in_channel,\n",
    "            channel,\n",
    "            n_res_block,\n",
    "            n_res_channel,\n",
    "            stride=4,\n",
    "        )\n",
    "\n",
    "    def forward(self, input,n_embedded_l, dim_l):\n",
    "        quant_t, quant_b, diff, _, _ = self.encode(input,n_embedded_l=n_embedded_l, dim_l=dim_l)\n",
    "        dec = self.decode(quant_t, quant_b)\n",
    "\n",
    "        return dec, diff\n",
    "\n",
    "    def encode(self, input,n_embedded_l, dim_l):\n",
    "        enc_b = self.enc_b(input)\n",
    "        enc_t = self.enc_t(enc_b)\n",
    "\n",
    "        quant_t = self.quantize_conv_t(enc_t).permute(0, 2, 3, 1)\n",
    "        quant_t, diff_t, id_t = self.quantize_t(quant_t, n_embedded_l=n_embedded_l, dim_l=dim_l )\n",
    "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
    "        diff_t = diff_t.unsqueeze(0)\n",
    "\n",
    "        dec_t = self.dec_t(quant_t)\n",
    "        enc_b = torch.cat([dec_t, enc_b], 1)\n",
    "\n",
    "        quant_b = self.quantize_conv_b(enc_b).permute(0, 2, 3, 1)\n",
    "        quant_b, diff_b, id_b = self.quantize_b(quant_b, n_embedded_l=n_embedded_l, dim_l=dim_l )\n",
    "\n",
    "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
    "        diff_b = diff_b.unsqueeze(0)\n",
    "\n",
    "        return quant_t, quant_b, diff_t + diff_b, id_t, id_b\n",
    "\n",
    "    def decode(self, quant_t, quant_b):\n",
    "        upsample_t = self.upsample_t(quant_t)\n",
    "        quant = torch.cat([upsample_t, quant_b], 1)\n",
    "        dec = self.dec(quant)\n",
    "\n",
    "        return dec\n",
    "\n",
    "    def encode_t(self, input, n_embedded_l=1, dim_l=1 ):\n",
    "        enc_b = self.enc_b(input)\n",
    "        enc_t = self.enc_t(enc_b)\n",
    "\n",
    "        quant_t = self.quantize_conv_t(enc_t).permute(0, 2, 3, 1)\n",
    "        quant_t, diff_t, id_t = self.quantize_t(quant_t, n_embedded_l=n_embedded_l, dim_l=dim_l )\n",
    "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
    "\n",
    "        return quant_t\n",
    "\n",
    "    def decode_code(self, code_t, code_b):\n",
    "        quant_t = self.quantize_t.embed_code(code_t)\n",
    "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
    "        quant_b = self.quantize_b.embed_code(code_b)\n",
    "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
    "\n",
    "        dec = self.decode(quant_t, quant_b)\n",
    "\n",
    "        return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9be49a64e3e131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T13:44:57.040216700Z",
     "start_time": "2023-10-13T13:44:56.954101600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "# model = VQVAE(in_channel=1,\n",
    "model =    Vqvae2Adaptive1(in_channel=3,\n",
    "                   channel=128,\n",
    "                   n_res_block=6,\n",
    "                   n_res_channel=32,\n",
    "                   embed_dim=8,\n",
    "                   n_embed=8192,\n",
    "                   decay=0.99).to(device)\n",
    "\n",
    "# model_file = 'data/logs/emb_dim_1_n_embed_8192_bc_right_sub_left_minmax_4x_360/vqvae_003_train_0.04287_test_0.04129.pt'\n",
    "model_file = 'data/logs/emb_dim_8_n_embed_8192_bc_left_9x_512_360/vqvae_002_train_0.02079_test_0.02053.pt'\n",
    "\n",
    "# model_file = 'data/logs/emb_dim_1_n_embed_8192_o_bc_left_9x_256_360_1/vqvae_003_train_0.01975_test_0.01967.pt'\n",
    "# model_file = 'data/logs/emb_dim_1_n_embed_8192_o_bc_left_9x_512_360_1/vqvae_003_train_0.01967_test_0.01958.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(model_file, map_location=torch.device('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de858bf63e80785",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T12:54:55.069469900Z",
     "start_time": "2023-10-13T12:54:54.932835800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = '../datasets/original/o_bc_left_9x_512_360'\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [   \n",
    "        # transforms.CenterCrop((256,256)),\n",
    "        # transforms.functional.invert,\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Grayscale(),\n",
    "        transforms.Normalize(0.5, 0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "\n",
    "dataset_path = dataset.__dict__['root']\n",
    "classes_folders = os.listdir(dataset_path)\n",
    "classes_folders_images = [os.listdir(dataset_path + '/' + folder) for folder in classes_folders]\n",
    "classes_folders_images_num = [len(os.listdir(dataset_path + '/' + folder)) for folder in classes_folders]\n",
    "img_transform = dataset.__dict__['transform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2ed9953cb897f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T13:03:10.279891700Z",
     "start_time": "2023-10-13T13:03:10.253961Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "j=0\n",
    "\n",
    "image_path = dataset_path + '/' + classes_folders[i] + '/' + classes_folders_images[i][j]\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image = img_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "quant_t= model.encode_t(image,n_embedded_l=-2, dim_l=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e989907b94fff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T12:02:13.413847900Z",
     "start_time": "2023-10-13T12:02:12.961944300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "quant_t, quant_b, diff, id_t, id_b = model.encode(image,n_embedded_l=-2, dim_l=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c53aeeb735b035",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T15:40:20.567137900Z",
     "start_time": "2023-10-13T15:40:17.414482900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "j=0\n",
    "\n",
    "image_path = dataset_path + '/' + classes_folders[i] + '/' + classes_folders_images[i][j]\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image = img_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "# dec, diff = model.forward(image,n_embedded_l=None, dim_l=None)\n",
    "\n",
    "for n_embedded in [None, -4096, -6144,-7000, -8000 ,-8190  ]:\n",
    "    for dim in [None, -1, -2, -3, -4, -5, -6, -7, -8]:\n",
    "        dec, diff = model(image,n_embedded_l=n_embedded, dim_l=dim)\n",
    "        \n",
    "        grid = utils.make_grid(dec, normalize=True)\n",
    "        ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
    "        \n",
    "        # io.imsave(f'{n_embedded},{dim}.png',ndarr)\n",
    "\n",
    "# plt.figure(figsize=(5,5))\n",
    "        plt.title(f'n_embedded={n_embedded}, dim={dim}')\n",
    "        plt.imshow(ndarr, cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b55771c93fb689",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T11:41:21.046738Z",
     "start_time": "2023-10-11T11:41:21.037595Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "S=np.load('S.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4bb6519413d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T11:52:02.789639Z",
     "start_time": "2023-10-11T11:51:52.881301Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "g=plt.hist(S, bins=10000)\n",
    "plt.xlim(0,20)\n",
    "plt.xlabel('Singular values')\n",
    "plt.ylabel('P(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a780f27748342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-11T11:49:27.120901Z",
     "start_time": "2023-10-11T11:49:26.995850Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hist, bin_edges=np.histogram(S, bins=10000)\n",
    "hist=hist/np.sum(hist)\n",
    "\n",
    "plt.xlim(0,50)\n",
    "plt.xlabel('Singular values')\n",
    "plt.ylabel('P(x)')\n",
    "\n",
    "plt.scatter(bin_edges[:-1], hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
