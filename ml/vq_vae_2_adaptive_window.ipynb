{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from skimage import io\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from scheduler import CycleScheduler\n",
    "from pt_utils import  Embeddings, Trainer, VQVAE, data_sampler\n",
    "from torch.utils import data\n",
    "from torch import distributed as dist\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from skimage import transform, metrics\n",
    "import skimage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "# from tqdm.notebook import trange, tqdm\n",
    "from tqdm import trange, tqdm\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from pt_utils import Encoder, Decoder, all_reduce\n",
    "from torch.nn import functional as F\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9909e6c890ed3e7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataset_path = '../datasets/bc_right_sub_left_minmax_4x_360'\n",
    "# dataset_path = '../datasets/bc_left_sub_right_0.5_4x_360'\n",
    "# dataset_path = '../datasets/bc_right_sub_left_0.5_4x_360'\n",
    "# dataset_path = '../datasets/bc_left_sub_right_minmax_4x_360'\n",
    "dataset_path = '../datasets/bc_left_4x_360'\n",
    "\n",
    "resize_shape = (512, 512)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.Resize(resize_shape),\n",
    "        transforms.CenterCrop(resize_shape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=transform)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5de858bf63e80785"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QuantizeAdaptive(nn.Module):\n",
    "    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.n_embed = n_embed\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "\n",
    "        embed = torch.randn(dim, n_embed)\n",
    "        self.register_buffer(\"embed\", embed)\n",
    "        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n",
    "        self.register_buffer(\"embed_avg\", embed.clone())\n",
    "\n",
    "    def forward(self, input, n_embedded_l=-1, din_l=-1):\n",
    "        print(input)\n",
    "        flatten = input.reshape(-1, self.dim)\n",
    "        dist = (\n",
    "                flatten.pow(2).sum(1, keepdim=True)\n",
    "                - 2 * flatten @ self.embed\n",
    "                + self.embed.pow(2).sum(0, keepdim=True)\n",
    "        )\n",
    "        _, embed_ind = (-dist).max(1)\n",
    "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
    "        embed_ind = embed_ind.view(*input.shape[:-1])\n",
    "        quantize = self.embed_code(embed_ind)\n",
    "\n",
    "        if self.training:\n",
    "            embed_onehot_sum = embed_onehot.sum(0)\n",
    "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
    "\n",
    "            all_reduce(embed_onehot_sum)\n",
    "            all_reduce(embed_sum)\n",
    "\n",
    "            self.cluster_size.data.mul_(self.decay).add_(\n",
    "                embed_onehot_sum, alpha=1 - self.decay\n",
    "            )\n",
    "            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n",
    "            n = self.cluster_size.sum()\n",
    "            cluster_size = (\n",
    "                    (self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n\n",
    "            )\n",
    "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "\n",
    "        diff = (quantize.detach() - input).pow(2).mean()\n",
    "        quantize = input + (quantize - input).detach()\n",
    "        print(quantize)\n",
    "        return quantize, diff, embed_ind\n",
    "\n",
    "    def embed_code(self, embed_id):\n",
    "        return F.embedding(embed_id, self.embed.transpose(0, 1))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd1a71342adf3048"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channel=3,\n",
    "            channel=128,\n",
    "            n_res_block=5,\n",
    "            n_res_channel=32,\n",
    "            embed_dim=64,\n",
    "            n_embed=512,\n",
    "            decay=0.99,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_b = Encoder(in_channel, channel, n_res_block, n_res_channel, stride=4)\n",
    "        self.enc_t = Encoder(channel, channel, n_res_block, n_res_channel, stride=2)\n",
    "        self.quantize_conv_t = nn.Conv2d(channel, embed_dim, 1)\n",
    "        self.quantize_t = Quantize(embed_dim, n_embed)\n",
    "        self.dec_t = Decoder(\n",
    "            embed_dim, embed_dim, channel, n_res_block, n_res_channel, stride=2\n",
    "        )\n",
    "        self.quantize_conv_b = nn.Conv2d(embed_dim + channel, embed_dim, 1)\n",
    "        self.quantize_b = Quantize(embed_dim, n_embed)\n",
    "        self.upsample_t = nn.ConvTranspose2d(\n",
    "            embed_dim, embed_dim, 4, stride=2, padding=1\n",
    "        )\n",
    "        self.dec = Decoder(\n",
    "            embed_dim + embed_dim,\n",
    "            in_channel,\n",
    "            channel,\n",
    "            n_res_block,\n",
    "            n_res_channel,\n",
    "            stride=4,\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        quant_t, quant_b, diff, _, _ = self.encode(input)\n",
    "        dec = self.decode(quant_t, quant_b)\n",
    "\n",
    "        return dec, diff\n",
    "\n",
    "    def encode(self, input):\n",
    "        enc_b = self.enc_b(input)\n",
    "        enc_t = self.enc_t(enc_b)\n",
    "\n",
    "        quant_t = self.quantize_conv_t(enc_t).permute(0, 2, 3, 1)\n",
    "        quant_t, diff_t, id_t = self.quantize_t(quant_t)\n",
    "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
    "        diff_t = diff_t.unsqueeze(0)\n",
    "\n",
    "        dec_t = self.dec_t(quant_t)\n",
    "        enc_b = torch.cat([dec_t, enc_b], 1)\n",
    "\n",
    "        quant_b = self.quantize_conv_b(enc_b).permute(0, 2, 3, 1)\n",
    "        quant_b, diff_b, id_b = self.quantize_b(quant_b)\n",
    "\n",
    "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
    "        diff_b = diff_b.unsqueeze(0)\n",
    "\n",
    "        return quant_t, quant_b, diff_t + diff_b, id_t, id_b\n",
    "\n",
    "    def decode(self, quant_t, quant_b):\n",
    "        upsample_t = self.upsample_t(quant_t)\n",
    "        quant = torch.cat([upsample_t, quant_b], 1)\n",
    "        dec = self.dec(quant)\n",
    "\n",
    "        return dec\n",
    "\n",
    "    def decode_code(self, code_t, code_b):\n",
    "        quant_t = self.quantize_t.embed_code(code_t)\n",
    "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
    "        quant_b = self.quantize_b.embed_code(code_b)\n",
    "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
    "\n",
    "        dec = self.decode(quant_t, quant_b)\n",
    "\n",
    "        return dec\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45e9067e6629cb76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "966cd4dc6174fe9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_path = dataset.__dict__['root']\n",
    "classes_folders = os.listdir(dataset_path)\n",
    "classes_folders_images = [os.listdir(dataset_path + '/' + folder) for folder in classes_folders]\n",
    "classes_folders_images_num = [len(os.listdir(dataset_path + '/' + folder)) for folder in classes_folders]\n",
    "img_transform = dataset.__dict__['transform']\n",
    "\n",
    "i=0\n",
    "j=0\n",
    "\n",
    "image_path = dataset_path + '/' + classes_folders[i] + '/' + classes_folders_images[i][j]\n",
    "\n",
    "image = Image.open(image_path)\n",
    "image = image.convert(\"RGB\")\n",
    "image = img_transform(image)\n",
    "image = image.unsqueeze(0).to(device)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "quant_t, quant_b, diff, _, indx_b = model.encode(image)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db2ed9953cb897f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
