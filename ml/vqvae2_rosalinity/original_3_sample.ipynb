{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ed29671-0057-4ff5-b4bf-bd6b48ea9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from vqvae import VQVAE\n",
    "from pixelsnail import PixelSNAIL\n",
    "import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from skimage import io, color\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b04ba50-eadd-42da-a02e-3bb757ef4c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_model(model, device, batch, size, temperature, condition=None):\n",
    "    row = torch.zeros(batch, *size, dtype=torch.int64).to(device)\n",
    "    cache = {}\n",
    "\n",
    "    for i in tqdm(range(size[0])):\n",
    "        for j in range(size[1]):\n",
    "            out, cache = model(row[:, : i + 1, :], condition=condition, cache=cache)\n",
    "            prob = torch.softmax(out[:, :, i, j] / temperature, 1)\n",
    "            sample = torch.multinomial(prob, 1).squeeze(-1)\n",
    "            row[:, i, j] = sample\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def load_model(model, checkpoint, device):\n",
    "\n",
    "    if model == 'vqvae':\n",
    "        model =    VQVAE(\n",
    "                        in_channel=3,\n",
    "                        channel=128,\n",
    "                        n_res_block=6,\n",
    "                        n_res_channel=32,\n",
    "                        embed_dim=1,\n",
    "                        n_embed=256,\n",
    "                        decay=0.99\n",
    "                        ).to(device)\n",
    "\n",
    "    elif model == 'pixelsnail_top':\n",
    "        model = PixelSNAIL(\n",
    "            [32, 32],\n",
    "            512,\n",
    "            channel,\n",
    "            5,\n",
    "            4,\n",
    "            n_res_block,\n",
    "            n_res_channel,\n",
    "            dropout=dropout,\n",
    "            n_out_res_block=n_out_res_block,\n",
    "        )\n",
    "\n",
    "    elif model == 'pixelsnail_bottom':\n",
    "        model = PixelSNAIL(\n",
    "            [64, 64],\n",
    "            512,\n",
    "            channel,\n",
    "            5,\n",
    "            4,\n",
    "            n_res_block,\n",
    "            n_res_channel,\n",
    "            attention=False,\n",
    "            dropout=dropout,\n",
    "            n_cond_res_block=n_cond_res_block,\n",
    "            cond_res_channel=n_res_channel,\n",
    "        )\n",
    "        \n",
    "    model.load_state_dict(torch.load(checkpoint),strict=False)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c988594-a1b3-41b5-81be-ff4e4c147266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_img(out):\n",
    "    utils.save_image(\n",
    "        out,\n",
    "        f\"test1.png\",\n",
    "        nrow=4,\n",
    "        normalize=True,\n",
    "        )\n",
    "    img=io.imread('test1.png')\n",
    "    os.remove('test1.png')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c77e262d-a9b8-4aad-9fc8-0681da55dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "n_res_block=4\n",
    "n_res_channel=256\n",
    "n_out_res_block=0\n",
    "n_cond_res_block=3\n",
    "channel=256\n",
    "dropout=0.1\n",
    "\n",
    "batch=1\n",
    "\n",
    "# vqvae='runs/pixelsnail_emb_dim_1_n_embed_256_bc_left_4x_768_zeros/bottom/vqvae_010_train_0.01976_test_0.01965.pt'\n",
    "# top='runs/14.04.2024/65_pixelsnail_top_train_loss_1.281939_acc_0.379639_test_loss_1.230041_acc_0.391327.pt'\n",
    "# bottom='runs/pixelsnail_emb_dim_1_n_embed_256_bc_left_4x_768_zeros/bottom/13_pixelsnail_bottom_train_loss_3.277416_acc_0.120651_test_loss_2.938762_acc_0.138824.pt'\n",
    "\n",
    "# dataset_path = '../diffusion/runs/wc_co_median_latent/samples/'\n",
    "# vqvae='../vqvae2_rosalinity/runs/emb_dim_1_n_embed_256_o_bc_left_4x_768_360_median/vqvae_006_train_0.00047_test_0.00053.pt'\n",
    "vqvae='runs_unique/emb_dim_1_n_embed_256_bc_left_4x_768_360_728/vqvae_004_train_0.00041_test_0.00039.pt'\n",
    "\n",
    "filename='test.png'\n",
    "temp=10\n",
    "\n",
    "model_vqvae = load_model('vqvae', vqvae, device)\n",
    "# model_top = load_model('pixelsnail_top', top, device)\n",
    "# model_bottom = load_model('pixelsnail_bottom', bottom, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4528837-931a-4807-a7e8-18c5ac024c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_sample = sample_model(model_top, device, batch, [32, 32], temp)\n",
    "top_sample_z=torch.zeros((1,64,64),dtype=torch.float32).to(device).unsqueeze(dim=0)\n",
    "\n",
    "# bottom_sample = sample_model(\n",
    "#     model_bottom, device, batch, [64, 64], temp, condition=top_sample_z\n",
    "# )\n",
    "\n",
    "names_new={'Ultra_Co11':'AB_Co11_medium',\n",
    "           'Ultra_Co15':'AB_Co15_medium_small',\n",
    "           'Ultra_Co25':'AB_Co25_small',\n",
    "           'Ultra_Co6_2':'AB_Co6_large',\n",
    "           'Ultra_Co8':'AB_Co_8_medium_small'}\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    # for j in range(4):\n",
    "        \n",
    "        # bottom_sample_o=color.rgb2gray(io.imread(f'../diffusion/embs_images_emb_dim_1_n_embed_256_o_bc_left_4x_768_360_768_median_Ultra_Co11/g/00{i}.png')[:128,128*j:128*(j+1)])\n",
    "        \n",
    "        bottom_sample_o=color.rgb2gray(io.imread(f'../diffusion/runs/embs_images_emb_dim_1_n_embed_256_o_bc_left_4x_768_360_768_median_Ultra_Co25/generated/{i}.png'))\n",
    "        \n",
    "        old_shape=copy.copy(bottom_sample_o.shape)\n",
    "        scaler=preprocessing.MinMaxScaler((-1.5,1.5))\n",
    "        scaler.fit(bottom_sample_o.reshape(-1, 1))\n",
    "        bottom_sample=scaler.transform(bottom_sample_o.reshape(-1, 1)).reshape(old_shape)\n",
    "\n",
    "        bottom_sample=torch.FloatTensor(bottom_sample).unsqueeze(dim=0).unsqueeze(dim=0).to(device)\n",
    "\n",
    "        model_vqvae.to('cuda')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            decoded_sample=model_vqvae.decode(top_sample_z, bottom_sample)\n",
    "\n",
    "        decoded_sample=convert_img(decoded_sample)\n",
    "\n",
    "        # plt.imshow(decoded_sample)\n",
    "        # plt.axis(\"off\")\n",
    "        # plt.savefig(f'{i}_{j}.png',bbox_inches='tight')\n",
    "        # plt.imsave(f'{i}_{j}.png', decoded_sample)\n",
    "        plt.imsave(f'latent_diff_dataset/Ultra_Co25/{i}.png', decoded_sample)\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c79d0236-7c94-45b3-b50f-ba048ff681db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_sample_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac6aeaa-fd2d-4b20-9234-0d3270bc78de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_sample_z=torch.zeros((1,64,64),dtype=torch.float32).to(device).unsqueeze(dim=0)\n",
    "\n",
    "bottom_sample_o=color.rgb2gray(io.imread(f'latent_medium_0099.jpg'))\n",
    "\n",
    "old_shape=copy.copy(bottom_sample_o.shape)\n",
    "scaler=preprocessing.MinMaxScaler((-1.5,1.5))\n",
    "scaler.fit(bottom_sample_o.reshape(-1, 1))\n",
    "bottom_sample=scaler.transform(bottom_sample_o.reshape(-1, 1)).reshape(old_shape)\n",
    "\n",
    "bottom_sample=torch.FloatTensor(bottom_sample).unsqueeze(dim=0).unsqueeze(dim=0).to(device)\n",
    "\n",
    "model_vqvae.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    decoded_sample=model_vqvae.decode(top_sample_z, bottom_sample)\n",
    "\n",
    "decoded_sample=convert_img(decoded_sample)\n",
    "\n",
    "# plt.imshow(decoded_sample)\n",
    "# plt.axis(\"off\")\n",
    "# plt.savefig(f'{i}_{j}.png',bbox_inches='tight')\n",
    "plt.imsave(f'latent_medium_0099_restored.jpeg', decoded_sample)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b6c02-3f55-4c26-87fb-eb616e2cfec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vqvae.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    decoded_sample=model_vqvae.decode(top_sample_z, bottom_sample)\n",
    "\n",
    "decoded_sample=convert_img(decoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9d4b8-78f0-442c-96d8-a165c2765409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(decoded_sample)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('ps.png',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa46f2-9662-4dd6-9c80-5d747bccfd93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=model_top\n",
    "device=device\n",
    "batch=5\n",
    "size=[32, 32]\n",
    "temp=1\n",
    "condition=None\n",
    "row = torch.zeros(batch, *size, dtype=torch.int64).to(device)\n",
    "cache = {}\n",
    "\n",
    "for i in tqdm(range(size[0])):\n",
    "    for j in range(size[1]):\n",
    "        out, cache = model(row[:, : i + 1, :], condition=condition, cache=cache)\n",
    "        prob = torch.softmax(out[:, :, i, j] / temp, 1)\n",
    "        sample = torch.multinomial(prob, 1).squeeze(-1)\n",
    "        row[:, i, j] = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b15003-014b-41cb-b06e-f732fdea9994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img=decoded_sample[0].cpu().detach().numpy()\n",
    "\n",
    "img=np.swapaxes(img, 0,-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
