{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2355b-5160-43e8-a847-673fc4266d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from skimage import io\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# from scheduler import CycleScheduler\n",
    "from pt_utils import  Embeddings, Trainer, VQVAE, data_sampler, Vqvae2Adaptive\n",
    "from torch.utils import data\n",
    "from torch import distributed as dist\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from skimage import transform, metrics\n",
    "import skimage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "seed = 51\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027b607-f89f-4bc7-b60a-52c073817e21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_path = '../datasets/bc_right_sub_left_minmax_4x_360'\n",
    "# dataset_path = '../datasets/bc_left_sub_right_0.5_4x_360'\n",
    "# dataset_path = '../datasets/bc_right_sub_left_0.5_4x_360'\n",
    "# dataset_path = '../datasets/bc_left_sub_right_minmax_4x_360'\n",
    "# dataset_path = '../datasets/bc_left_4x_360'\n",
    "dataset_path = '../../datasets/original/o_bc_left_4x_768_360/'\n",
    "\n",
    "resize_shape = (512, 512)\n",
    "# resize_shape = (256, 256)\n",
    "\n",
    "val_split=0.15\n",
    "batch_size=8\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.Resize(resize_shape),\n",
    "        transforms.CenterCrop(resize_shape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "\n",
    "train_dataset_len = int(len(dataset) * (1 - val_split))\n",
    "test_dataset_len = len(dataset) - train_dataset_len\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_dataset_len, test_dataset_len],\n",
    "                                                            # generator=torch.Generator().manual_seed(seed)\n",
    "                                                           )\n",
    "\n",
    "train_sampler = data_sampler(train_dataset, shuffle=True, distributed=False)\n",
    "test_sampler = data_sampler(test_dataset, shuffle=True, distributed=False)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, sampler=test_sampler, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa9b6a-eab9-4c9a-b8a1-5e0375c87688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_file = 'runs/emb_dim_2_n_embed_512_bc_left_4x_768_360/vqvae_012_train_0.0177_test_0.01751.pt'\n",
    "# model_file = 'runs/emb_dim_1_n_embed_512_bc_left_4x_768_360/vqvae_008_train_0.01965_test_0.02003.pt'\n",
    "# model_file = 'runs/emb_dim_1_n_embed_512_bc_left_4x_768_360/vqvae_002_train_0.02063_test_0.02073.pt'\n",
    "model_file = 'runs/emb_dim_1_n_embed_256_bc_left_4x_768_360/vqvae_005_train_0.0199_test_0.01966.pt'\n",
    "# model_file = 'runs/emb_dim_8_n_embed_256_bc_left_4x_768_360/vqvae_003_train_0.021_test_0.02086.pt'\n",
    "# model_file = 'runs/emb_dim_2_n_embed_256_bc_left_4x_768_360/vqvae_002_train_0.02079_test_0.01971.pt'\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model =    VQVAE(\n",
    "                in_channel=3,\n",
    "                channel=128,\n",
    "                n_res_block=2,\n",
    "                n_res_channel=32,\n",
    "                embed_dim=1,\n",
    "                n_embed=256,\n",
    "                decay=0.99\n",
    "                   ).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6bd5e-5512-4209-a51c-b558502307b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_imgs=[]\n",
    "images_embs_t = []\n",
    "images_embs_b = []\n",
    "indices_t=[]\n",
    "indices_b=[]\n",
    "labels=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "\n",
    "        batch_tensor=torch.FloatTensor(batch[0]).to(device)\n",
    "        quant_t, quant_b, diff, ind_t, ind_b = model.encode(batch_tensor)\n",
    "        \n",
    "        # original_imgs.extend(batch_tensor.cpu().detach().numpy())\n",
    "        images_embs_t.extend(quant_t.cpu().detach().numpy())\n",
    "        images_embs_b.extend(quant_b.cpu().detach().numpy())\n",
    "        indices_t.extend(ind_t.cpu().detach().numpy())\n",
    "        indices_b.extend(ind_b.cpu().detach().numpy())\n",
    "        labels.extend(batch[1].cpu().detach().numpy())\n",
    "\n",
    "\n",
    "# original_imgs=np.array(original_imgs)\n",
    "images_embs_t=np.array(images_embs_t)\n",
    "images_embs_b=np.array(images_embs_b)\n",
    "indices_t=np.array(indices_t)\n",
    "indices_b=np.array(indices_b)\n",
    "labels=np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d48f6-5df3-4d58-bf2f-6aac1d1c57ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_indices=np.argsort(labels)\n",
    "\n",
    "# original_imgs_sorted=original_imgs[sorted_indices]\n",
    "images_embs_t_sorted=images_embs_t[sorted_indices]\n",
    "images_embs_b_sorted=images_embs_b[sorted_indices]\n",
    "indices_t_sorted=indices_t[sorted_indices]\n",
    "indices_b_sorted=indices_b[sorted_indices]\n",
    "\n",
    "n=len(np.where(labels[sorted_indices]==0)[0])-1\n",
    "shape1=images_embs_t[0].shape\n",
    "shape2=images_embs_b[0].shape\n",
    "\n",
    "k=2\n",
    "\n",
    "images_embs_t_sorted=images_embs_t_sorted.reshape((5,1224,1,k*32,k*32))\n",
    "images_embs_b_sorted=images_embs_b_sorted.reshape((5,1224,1,k*64,k*64))\n",
    "indices_t_sorted=indices_t_sorted.reshape((5,1224,1,k*32,k*32))\n",
    "indices_b_sorted=indices_b_sorted.reshape((5,1224,1,k*64,k*64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf08f92-6dcb-4b85-9796-235c33bdcf80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_img(out):\n",
    "    utils.save_image(\n",
    "        out,\n",
    "        f\"test1.png\",\n",
    "        nrow=4,\n",
    "        normalize=True,\n",
    "        )\n",
    "    img=io.imread('test1.png')\n",
    "    os.remove('test1.png')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e8f69-334c-4efc-b660-c144b87f1757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(img.flatten(), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58171f78-bfb8-4efe-8960-5f48f6b72577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(quant_b.cpu().detach().numpy().flatten(), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db91af7-2100-4b90-830d-0ce88064b119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quant_t_[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2441a-f6a1-4151-a13c-60e709baee7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_indices=np.argsort(labels)\n",
    "\n",
    "sorted_labels=labels[sorted_indices]\n",
    "indices=[np.where(sorted_labels==i) for i in range(5)]\n",
    "\n",
    "cls_1=0\n",
    "key_1=4\n",
    "\n",
    "cls_2=0\n",
    "key_2=4\n",
    "\n",
    "quant_t_=images_embs_t_sorted[cls_1,key_1]\n",
    "# quant_t_=np.random.normal(0,0.5, 64*64).reshape((1,64,64))\n",
    "# quant_t=np.random.geometric(0.2,64*64).reshape((1,64,64))\n",
    "quant_b_=images_embs_b_sorted[cls_2,key_2]\n",
    "# quant_b=np.random.normal(0,0.5, 128*128).reshape((1,128,128))\n",
    "\n",
    "quant_t=torch.FloatTensor(quant_t_).to(device)\n",
    "quant_b=torch.FloatTensor(quant_b_).to(device)\n",
    "\n",
    "quant_t=torch.unsqueeze(quant_t, dim=0)\n",
    "quant_b=torch.unsqueeze(quant_b, dim=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    img=model.decode(quant_t, quant_b)\n",
    "\n",
    "img=convert_img(img)\n",
    "\n",
    "fig, axes=plt.subplots(1,3, figsize=(20,10))\n",
    "\n",
    "\n",
    "\n",
    "axes[0].imshow(img)\n",
    "# axes[1].imshow(indices_t_sorted[cls_1,key_1].swapaxes(0,-1))\n",
    "axes[1].imshow(quant_t_.swapaxes(0,-1), cmap='gray')\n",
    "# axes[2].imshow(indices_b_sorted[cls_2,key_2].swapaxes(0,-1), )\n",
    "axes[2].imshow(np.flip(np.rot90(quant_b_.swapaxes(0,-1),-1),1), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf4ec8-51cd-4024-a17b-e49a6ec6037a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices_t_sorted[cls_1,key_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a855b7f-d3aa-46ef-80cc-9e230e386145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images_embs_t_sorted[cls_1,key_1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1310a1c4-1288-4d49-be0b-30b2d252cc21",
   "metadata": {},
   "source": [
    "## 1. $C^1_{top}$ + $C^2_{bottom}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174e5f9-90bb-4fcd-a71b-b4c57073d2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "517fa99d-805b-4dea-8b6a-bf3ae85ac67e",
   "metadata": {},
   "source": [
    "## 2. $C^1_{top/bottom}$ + random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e166b-e32e-4da9-bfb9-807fff1d4010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af3783c2-5891-4d88-a54d-2d0d5876b6b5",
   "metadata": {},
   "source": [
    "## 2. $C^1_{top}$ + sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfbbd8-578d-4818-b770-1f2f30935227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
